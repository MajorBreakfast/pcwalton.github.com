<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[pcwalton]]></title>
  <link href="http://pcwalton.github.com/atom.xml" rel="self"/>
  <link href="http://pcwalton.github.com/"/>
  <updated>2017-02-14T13:03:39-08:00</updated>
  <id>http://pcwalton.github.com/</id>
  <author>
    <name><![CDATA[Patrick Walton]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Pathfinder, a fast GPU-based font rasterizer in Rust]]></title>
    <link href="http://pcwalton.github.com/blog/2017/02/14/pathfinder/"/>
    <updated>2017-02-14T11:03:00-08:00</updated>
    <id>http://pcwalton.github.com/blog/2017/02/14/pathfinder</id>
    <content type="html"><![CDATA[<p>Ever since some initial discussions with <a href="http://levien.com/">Raph Levien</a> (author of <code>font-rs</code>) at RustConf last
September, I&#8217;ve been thinking about ways to improve vector graphics rendering using modern graphics
hardware, specifically for fonts. These ideas began to come together in December, and over the past
couple of months I&#8217;ve been working on actually putting them into a real, usable library. They&#8217;ve
proved promising, and now I have some results to show.</p>

<p>Today I&#8217;m pleased to announce <a href="https://github.com/pcwalton/pathfinder">Pathfinder</a>, a Rust library for OpenType font rendering. The goal is
nothing less than to be the fastest vector graphics renderer in existence, and the results so far
are extremely encouraging. Not only is it very fast according to the traditional metric of raw
rasterization performance, it&#8217;s <em>practical</em>, featuring very low setup time (end-to-end time
superior to the best CPU rasterizers), best-in-class rasterization performance even at small glyph
sizes, minimal memory consumption (both on CPU and GPU), compatibility with existing font formats,
portability to most graphics hardware manufactured in the past five years (DirectX 10 level), and
security/safety.</p>

<h2>Performance</h2>

<p>To illustrate what it means to be both practical and fast, consider these two graphs:</p>

<p><a href="http://pcwalton.github.com/images/post-images/pathfinder-rasterize.svg"><!--
--><img alt="Rasterization performance" src="http://pcwalton.github.com/images/post-images/pathfinder-rasterize.svg"><!--
--></a></p>

<p><a href="http://pcwalton.github.com/images/post-images/pathfinder-setup.svg"><!--
--><img alt="Setup performance" src="http://pcwalton.github.com/images/post-images/pathfinder-setup.svg"><!--
--></a></p>

<p>(Click each graph for a larger version.)</p>

<p>The first graph is a comparison of Pathfinder with other rasterization algorithms with all vectors
already prepared for rendering (and uploaded to the GPU, in the case of the GPU algorithms). The
second graph is the total time taken to prepare and rasterize a glyph at a typical size, measured
from the point right after loading the OTF file in memory to the completion of rasterization. Lower
numbers are better. All times were measured on a Haswell Intel Iris Pro (mid-2015 MacBook Pro).</p>

<p>From these graphs, we can see two major problems with existing GPU-based approaches:</p>

<ol>
<li><p><em>Many algorithms aren&#8217;t that fast, especially at small sizes.</em> Algorithms aren&#8217;t fast just
because they run on the GPU! In general, we want rendering on the GPU to be faster than
rendering on the CPU; that&#8217;s often easier said than done, because modern CPUs are surprisingly
speedy. (Note that, even if the GPU is somewhat slower at a task than the CPU, it may be a win for
CPU-bound apps to offload some work; however, this makes the use of the algorithm highly
situational.) It&#8217;s much better to have an algorithm that actually beats the CPU.</p></li>
<li><p><em>Long setup times can easily eliminate the speedup of algorithms in practice.</em> This is known as
the &#8220;end-to-end&#8221; time, and real-world applications must carefully pay attention to it. One of
the most common use cases for a font rasterizer is to open a font file, rasterize a character set
from it (Latin-1, say) at one pixel size for later use, and throw away the file. With Web fonts now
commonplace, this use case becomes even more important, because Web fonts are frequently rasterized
once and then thrown away as the user navigates to a new page. Long setup times, whether the result
of tessellation or more exotic approaches, are real problems for these scenarios, since what the
user cares about is the document appearing quickly. Faster rasterization doesn&#8217;t help if it
regresses that metric.</p></li>
</ol>


<p>(Of the two problems mentioned above, the second is often totally ignored in the copious literature
on GPU-based vector rasterization. I&#8217;d like to see researchers start to pay attention to it. In
most scenarios, we don&#8217;t have the luxury of inventing our own GPU-friendly vector format. We&#8217;re not
going to get the world to move away from OpenType and SVG.)</p>

<h2>Vector drawing basics</h2>

<p>In order to understand the details of the algorithm, some basic knowledge of vector graphics is
necessary. Feel free to skip this section if you&#8217;re already familiar with Bézier curves and fill
rules.</p>

<p>OpenType fonts are defined in terms of resolution-independent <a href="https://en.wikipedia.org/wiki/B%C3%A9zier_curve">Bézier curves</a>. TrueType outlines
contain lines and quadratic Béziers only, while OpenType outlines can contain lines, quadratic
Béziers, and cubic Béziers. (Right now, Pathfinder only supports quadratic Béziers, but extending
the algorithm to support cubic Béziers should be straightforward.)</p>

<p>In order to fill vector paths, we need a <em>fill rule</em>. A fill rule is essentially a test that
determines, for every point, whether that point is inside or outside the curve (and therefore
whether it should be filled in). OpenType&#8217;s fill rule is the <em><a href="https://en.wikipedia.org/wiki/Nonzero-rule">winding rule</a></em>, which can be
expressed as follows:</p>

<ol>
<li><p>Pick a point that we want to determine the color of. Call it P.</p></li>
<li><p>Choose any point outside the curve. (This is easy to determine since any point outside the
bounding box of the curve is trivially outside the curve.) Call it Q.</p></li>
<li><p>Let the <em>winding number</em> be 0.</p></li>
<li><p>Trace a straight line from Q to P. Every time we cross a curve going clockwise, add 1 to the
winding number. Every time we cross a curve going counterclockwise, subtract 1 from the winding
number.</p></li>
<li><p>The point is inside the curve (and so should be filled) if and only if the winding number is not
zero.</p></li>
</ol>


<h2>How it works, conceptually</h2>

<p>The basic algorithm that Pathfinder uses is the by-now-standard trapezoidal pixel coverage
algorithm pioneered by Raph Levien&#8217;s libart (to the best of my knowledge). Variations of it are
used in FreeType, <a href="http://nothings.org/gamedev/rasterize/"><code>stb_truetype</code> version 2.0 and up</a>, and <a href="https://medium.com/@raphlinus/inside-the-fastest-font-renderer-in-the-world-75ae5270c445"><code>font-rs</code></a>. These implementations
differ as to whether they use sparse or dense representations for the coverage buffer. Following
<code>font-rs</code>, and unlike FreeType and <code>stb_truetype</code>, Pathfinder uses a dense representation for
coverage. As a result, <a href="https://medium.com/@raphlinus/inside-the-fastest-font-renderer-in-the-world-75ae5270c445">Raph&#8217;s description of the algorithm</a> applies fairly well to Pathfinder as
well.</p>

<p>There are two phases to the algorithm: <em>drawing</em> and <em>accumulation</em>. During the draw phase,
Pathfinder computes <em>coverage deltas</em> for every pixel touching (or immediately below) each curve.
During the accumulation phase, the algorithm sweeps down each column of pixels, computing
winding numbers (fractional winding numbers, since we&#8217;re antialiasing) and filling pixels
appropriately.</p>

<p>The most important concept to understand is that of the coverage delta. When drawing high-quality
antialiased curves, we care not only about whether each pixel is inside or outside the curve but
also <em>how much</em> of the pixel is inside or outside the curve. We treat each pixel that a curve
passes through as a small square and compute how much of the square the curve occupies. Because we
break down curves into small lines before rasterizing them, these coverage areas are always
trapezoids or triangles, and so and so we can use trapezoidal area expressions to calculate them.
The exact formulas involved are somewhat messy and involve several special cases; see <a href="http://nothings.org/gamedev/rasterize/">Sean
Barrett&#8217;s description of the <code>stb_truetype</code> algorithm</a> for the details.</p>

<p>Rasterizers that calculate coverage in this way differ in whether they calculate winding numbers
and fill at the same time they calculate coverage or whether they fill in a separate step after
coverage calculation. Sparse implementations like FreeType and <code>stb_truetype</code> usually fill as they
go, while dense implementations like <code>font-rs</code> and Pathfinder fill in a separate step. Filling in a
separate step is attractive because it can be simplified to a <a href="https://en.wikipedia.org/wiki/Prefix_sum">prefix sum</a> over each pixel column
if we store the coverage for each pixel as <em>the difference between the coverage of the pixel and
the coverage of the pixel above it</em>. In other words, instead of determining the area of each pixel
that a curve covers, for each pixel we determine how much <em>additional</em> area the curve covers,
relative to the coverage area of the immediately preceding pixel.</p>

<p>This modification has the very attractive property that <em>all coverage deltas both inside and
outside the curve are zero</em>, since points completely inside a curve contribute no <em>additional</em> area
(except for the first pixel completely inside the curve). This property is key to Pathfinder&#8217;s
performance relative to most vector texture algorithms. Calculating exact area coverage is slow,
but calculating coverage deltas instead of absolute coverage essentially allows us to limit the
expensive calculations to the <em>edges</em> of the curve, reducing the amount of work the GPU has to do
to a fraction of what it would have to do otherwise.</p>

<p>In order to fill the outline and generate the final glyph image, we simply have to sweep down each
column of pixels, calculating the running total of area coverage and writing pixels according to
the winding rule. The formula to determine the color of each pixel is simple and fast:
<code>min(|coverage total so far|, 1.0)</code> (where 0.0 is a fully transparent pixel, 1.0 is a fully opaque
pixel, and values in between are different shades). Importantly, all columns are completely
independent and can be calculated in parallel.</p>

<h2>Implementation details</h2>

<p>With the advanced features in OpenGL 4.3, this algorithm can be straightforwardly adapted to the
GPU.</p>

<ol>
<li><p>As an initialization step, we create a <em>coverage buffer</em> to hold delta coverage values. This
coverage buffer is a single-channel floating-point framebuffer. We always draw to the
framebuffer with blending enabled (<code>GL_FUNC_ADD</code>, both source and destination factors set to
<code>GL_ONE</code>).</p></li>
<li><p>We expand the TrueType outlines from the variable-length compressed <code>glyf</code> format inside the
font to a fixed-length, but still compact, representation. This is necessary to be able to
operate on vertices in parallel, since variable-length formats are inherently sequential. These
outlines are then uploaded to the GPU.</p></li>
<li><p>Next, we draw each curve as a <em>patch</em>. In a tessellation-enabled drawing pipeline like the one
that Pathfinder uses, rather than submitting triangles directly to the GPU, we submit abstract
patches which are converted into triangles in hardware. We use indexed drawing (<code>glDrawElements</code>)
to take advantage of the GPU&#8217;s <a href="https://www.khronos.org/opengl/wiki/Post_Transform_Cache">post-transform cache</a>, since most vertices belong to two curves.</p></li>
<li><p>For each path segment that represents a Bézier curve, we tessellate the Bézier curve into a
series of small lines on the GPU. Then we expand all lines out to screen-aligned quads
encompassing their bounding boxes. (There is a complication here when these quads overlap; we may
have to generate extra one-pixel-wide quads here and strip them out with backface culling. See the
comments inside the tessellation control shader for details.)</p></li>
<li><p>In the fragment shader, we calculate trapezoidal coverage area for each pixel and write it to
the coverage buffer. This completes the draw step.</p></li>
<li><p>To perform the accumulation step, we attach the coverage buffer and the destination texture to
images. We then dispatch a simple compute shader with one invocation per pixel column. For each
row, the shader reads from the coverage buffer and writes the total coverage so far to the
destination texture. The <code>min(|coverage total so far, 1.0)</code> expression above need not be computed
explicitly, because our unsigned normalized atlas texture stores colors in this way automatically.</p></li>
</ol>


<p>The performance characteristics of this approach are excellent. No CPU preprocessing is needed
other than the conversion of the variable-length TrueType outline to a fixed-length format. The
number of draw calls is minimal—any number of glyphs can be rasterized in one draw call, even from
different fonts—and the depth and stencil buffers remain unused. Because the tessellation is
performed on the fly instead of on the CPU, the amount of data uploaded to the GPU is minimal. Area
coverage is essentially only calculated for pixels on the <em>edges</em> of the outlines, avoiding
expensive fragment shader invocations for all the pixels inside each glyph. The final accumulation
step has ideal characteristics for GPU compute, since branch divergence is nonexistent and cache
locality is maximized. All pixels in the final buffer are only painted at most once, regardless of
the number of curves present.</p>

<h2>Compatibility concerns</h2>

<p>For any GPU code designed to be shipping to consumers, especially OpenGL 3.0 and up, compatibility
and portability are always concerns. As Pathfinder is designed for OpenGL 4.3, released in 2012, it
is no exception. Fortunately, the algorithm can be adapted in various ways depending on the
available functionality.</p>

<ul>
<li><p>When compute shaders are not available (OpenGL 4.2 or lower), Pathfinder uses OpenCL 1.2 instead.
This is the case on the Mac, since Apple has not implemented any OpenGL features newer than
OpenGL 4.2 (2011). The <a href="https://github.com/pcwalton/compute-shader">compute-shader</a> crate abstracts over the subset of OpenGL and OpenCL
necessary to access GPU compute functionality.</p></li>
<li><p>When tessellation shaders are not available (OpenGL 3.3 or lower), Pathfinder uses geometry
shaders, available in OpenGL 3.2 and up.</p></li>
</ul>


<p>(Note that it should be possible to avoid both geometry shaders and tessellation shaders, at the
cost of performing that work on the CPU. This turns out to be quite fast. However, since image
load/store is a hard requirement, this seems pointless: both image load/store and geometry shaders
were introduced in DirectX 10-level hardware.)</p>

<p>Although these system requirements may seem high at first, the integrated graphics found in any
Intel Sandy Bridge (2011) CPU or later meet them.</p>

<h2>Future directions</h2>

<p>The immediate next step for Pathfinder is to integrate into WebRender as an optional accelerated
path for applicable fonts on supported GPUs. Beyond that, there are several features that could
be added to extend Pathfinder itself.</p>

<ol>
<li><p><em>Support vector graphics outside the font setting.</em> As Pathfinder is a generic vector graphics
rasterizer, it would be interesting to expose an API allowing it to be used as the backend for
e.g. an SVG renderer. Rendering the entire SVG specification is outside of the scope of Pathfinder
itself, but it could certainly be the path rendering component of a full SVG renderer.</p></li>
<li><p><em>Support CFF and CFF2 outlines.</em> These have been seen more and more over time, e.g. in Apple&#8217;s
new San Francisco font. Adding this support involves both parsing and extracting the <a href="https://www.microsoft.com/typography/otspec/cff2.htm">CFF2
format</a> and adding support for cubic Bézier curves to Pathfinder.</p></li>
<li><p><em>Support WOFF and WOFF2.</em> In the case of WOFF2, this involves writing a parser for the
transformed <code>glyf</code> table.</p></li>
<li><p><em>Support subpixel antialiasing.</em> This should be straightforward.</p></li>
<li><p><em>Support emoji</em>. The Microsoft <code>COLR</code> and Apple <code>sbix</code> extensions are straightforward, but the
Google <code>SVG</code> table allows arbitrary SVGs to be embedded into a font. Full support for SVG is
probably out of scope of Pathfinder, but perhaps the subset used in practice is small enough to
support.</p></li>
<li><p><em>Optimize overlapping paths.</em> It would be desirable to avoid antialiasing edges that are covered
by other paths. The fill rule makes this trickier than it initially sounds.</p></li>
<li><p><em>Support hinting.</em> This is low-priority since it&#8217;s effectively obsolete with high-quality
antialiasing, subpixel AA, and high-density displays, but it might be useful to match the system
rendering on Windows.</p></li>
</ol>


<h2>Conclusion</h2>

<p>Pathfinder is <a href="https://github.com/pcwalton/pathfinder">available on GitHub</a> and should be easily buildable using the stable version of Rust
and Cargo.  Please feel free to check it out, build it, and report bugs! I&#8217;m especially interested
in reports of poor performance, crashes, or rendering problems on a variety of hardware. As
Pathfinder does use DirectX 10-level hardware features, some amount of driver pain is unavoidable.
I&#8217;d like to shake these problems out as soon as possible.</p>

<p>Finally, I&#8217;d like to extend a special thanks to Raph Levien for many fruitful discussions and
ideas. This project wouldn&#8217;t have been possible without his insight and expertise.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Drawing CSS Box Shadows in WebRender]]></title>
    <link href="http://pcwalton.github.com/blog/2015/12/21/drawing-css-box-shadows-in-webrender/"/>
    <updated>2015-12-21T11:08:00-08:00</updated>
    <id>http://pcwalton.github.com/blog/2015/12/21/drawing-css-box-shadows-in-webrender</id>
    <content type="html"><![CDATA[<p>I recently landed a <a href="https://github.com/glennw/webrender/commit/d57057470cb2bddf0c8ece3fc29cfbe5d03114a2">change</a> in WebRender to draw <a href="https://developer.mozilla.org/en-US/docs/Web/CSS/box-shadow">CSS box shadows</a> using a specialized shader. Because it&#8217;s an unusual approach to drawing shadows, I thought I&#8217;d write up how it works.</p>

<p>Traditionally, browsers have drawn box shadows <a href="https://dxr.mozilla.org/mozilla-central/source/gfx/thebes/gfxBlur.cpp#624">in three passes</a>: (1) draw the unblurred box (or a nine-patch corner/edge for one); (2) blur in the horizontal direction; (3) blur in the vertical direction. This works because a Gaussian blur is a <a href="https://en.wikipedia.org/wiki/Separable_filter">separable filter</a>: it can be computed as the product of two one-dimensional convolutions. This is a reasonable approach, but it has downsides. First of all, it has a high cost in memory bandwidth; for a standard triple box blur on the CPU, every pixel is touched 6 times, and on the GPU every pixel is touched <code>$$6 sigma$$</code> times (or <code>$$3 sigma$$</code> times if a common linear interpolation trick is used), not counting the time needed to draw the unblurred image in the first place. (<code>$$sigma$$</code> here is <a href="http://dbaron.org/log/20110225-blur-radius">half the specified blur radius</a>.) Second, the painting of each box shadow requires no fewer than three draw calls including (usually) one shader switch, which are expensive, especially on mobile GPUs. On the GPU, it&#8217;s often desirable to use parallel algorithms that reduce the number of draw calls and state changes, even if those algorithms have a large number of raw floating-point operations—simply because the GPU is a stream processor that&#8217;s designed for such workloads.</p>

<p>The key trick used in WebRender is to take advantage of the fact that we&#8217;re blurring a (potentially rounded) <em>box</em>, not an ordinary image. This allows us to express the Gaussian blur in (in the case of an unrounded box) a closed form and (in the case of a rounded box) a closed form minus a sum computed with a small loop. To draw a box shadow, WebRender runs a shader implementing this logic and caches the results in a nine-patch image mask stored in a texture atlas. If the page contains multiple box shadows (even those with heterogeneous sizes and radii), the engine batches all the invocations of this shader into one draw call. This means that, no matter how many box shadows are in use, the number of draw calls and state changes remains constant (as long as the size of the texture atlas isn&#8217;t exhausted). Driver overhead and memory bandwidth are minimized, and the GPU spends as much time as possible in raw floating-point computation, which is exactly the kind of workload it&#8217;s optimized for.</p>

<p>The remainder of this post will be a dive into the logic of the fragment shader itself. The <a href="https://github.com/glennw/webrender/blob/d57057470cb2bddf0c8ece3fc29cfbe5d03114a2/res/box_shadow.fs.glsl">source code</a> may be useful as a reference.</p>

<p>For those unfamiliar with OpenGL, per-pixel logic is expressed with a <em>fragment shader</em> (sometimes called a <em>pixel shader</em>). A fragment shader (in this case) is conceptually a function that maps arbitrary per-pixel input data to the RGB values defining the resulting color for that pixel. In our case, the input data for each pixel simply consists of the <code>$$x$$</code> and <code>$$y$$</code> coordinates for that pixel. We&#8217;ll call our function <code>$$RGB(u,v)$$</code> and define it as follows:</p>

<pre><code>$$RGB(u,v) = sum_{y=-oo}^{oo} sum_{x=-oo}^{oo}G(x-u)G(y-v)RGB_{"rounded box"}(x,y)$$
</code></pre>

<p>Here, <code>$$RGB_{"rounded box"}(x,y)$$</code> is the color of the unblurred, possibly-rounded box at the coordinate <code>$$(x,y)$$</code>, and <code>$$G(x)$$</code> is the Gaussian function used for the blur:</p>

<pre><code>$$G(x)=1/sqrt(2 pi sigma^2) e^(-x^2/(2 sigma^2))$$
</code></pre>

<p>A Gaussian blur in one dimension is a convolution that maps each input pixel to an average of the pixels adjacent to it weighted by <code>$$G(x)$$</code>, where <code>$$x$$</code> is the distance from the output pixel. A two-dimensional Gaussian blur is simply the product of two one-dimensional Gaussian blurs, one for each dimension. This definition leads to the formula for <code>$$RGB(x,y)$$</code> above.</p>

<p>Since CSS box shadows blur solid color boxes, the color of each pixel is either the color of the shadow (call it <code>$$RGB_{"box"}$$</code>) or transparent. We can rewrite this into two functions:</p>

<pre><code>$$RGB(x,y) = RGB_{"box"}C(x,y)$$
</code></pre>

<p>and</p>

<pre><code>$$C(u,v) = sum_{y=-oo}^{oo} sum_{x=-oo}^{oo}G(x-u)G(y-v)C_{"rounded box"}(x,y)$$
</code></pre>

<p>where <code>$$C_{"rounded box"}(x,y)$$</code> is 1.0 if the point $$(x,y)$$ is inside the unblurred, possibly-rounded box and 0.0 otherwise.</p>

<p>Now let&#8217;s start with the simple case, in which the box is unrounded. We&#8217;ll call this function <code>$$C_{"blurred box"}$$</code>:</p>

<pre><code>$$C_{"blurred box"}(u,v) = sum_{y=-oo}^{oo} sum_{x=-oo}^{oo}G(x-u)G(y-v)C_{"box"}(x,y)$$
</code></pre>

<p>where <code>$$C_{"box"}(x,y)$$</code> is 1.0 if the point $$(x,y)$$ is inside the box and 0.0 otherwise.</p>

<p>Let <code>$$x_{"min"}, x_{"max"}, y_{"min"}, y_{"max"}$$</code> be the left, right, top, and bottom extents of the box respectively. Then <code>$$C_{"box"}(x,y)$$</code> is 1.0 if <code>$$x_{"min"} &lt;= x &lt;= x_{"max"}$$</code> and <code>$$y_{"min"} &lt;= y &lt;= y_{"max"}$$</code> and 0.0 otherwise. Now let&#8217;s rearrange <code>$$C_{"blurred box"}(x,y)$$</code> above:</p>

<pre><code>$$C_{"blurred box"}(u,v) =
    (sum_{y=-oo}^{y_{"min"} - 1}
        sum_{x=-oo}^{x=oo} G(x-u)G(y-v)C_{"box"}(x,y)) +
    (sum_{y=y_{"min"}}^{y_{"max"}}
        (sum_{x=-oo}^{x_{"min"}-1} G(x-u)G(y-v)C_{"box"}(x,y)) +
        (sum_{x=x_{"min"}}^{x_{"max"}} G(x-u)G(y-v)C_{"box"}(x,y)) +
        (sum_{x=x_{"max"}+1}^{x=oo} G(x-u)G(y-v)C_{"box"}(x,y))) +
    (sum_{y=y_{"max"} + 1}^{oo}
        sum_{x=-oo}^{x=oo} G(x)G(y)C_{"box"}(x,y))$$
</code></pre>

<p>We can now eliminate several of the intermediate sums, along with <code>$$C_{"box"}(x,y)$$</code>, using its definition and the sum bounds:</p>

<pre><code>$$C_{"blurred box"}(u,v) = sum_{y=y_{"min"}}^{y_{"max"}} sum_{x=x_{"min"}}^{x_{"max"}} G(x-u)G(y-v)$$
</code></pre>

<p>Now let&#8217;s simplify this expression to a closed form. To begin with, we&#8217;ll approximate the sums with integrals:</p>

<pre><code>$$C_{"blurred box"}(u,v) ~~ int_{y_{"min"}}^{y_{"max"}} int_{x_{"min"}}^{x_{"max"}} G(x-u)G(y-v) dxdy$$

$$= int_{y_{"min"}}^{y_{"max"}} G(y-v) int_{x_{"min"}}^{x_{"max"}} G(x-u) dxdy$$
</code></pre>

<p>Now the inner integral can be evaluated to a closed form:</p>

<pre><code>$$int_{x_{"min"}}^{x_{"max"}}G(x-u)dx
    = int_{x_{"min"}}^{x_{"max"}}1/sqrt(2 pi sigma^2) e^(-(x-u)^2/(2 sigma^2))dx
    = 1/2 "erf"((x_{"max"}-u)/(sigma sqrt(2))) - 1/2 "erf"((x_{"min"}-u)/(sigma sqrt(2)))$$
</code></pre>

<p><code>$$"erf"(x)$$</code> here is the <a href="https://en.wikipedia.org/wiki/Error_function">Gauss error function</a>. It is not found in GLSL (though it is found in <code>&lt;math.h&gt;</code>), but it does have the following <a href="https://en.wikipedia.org/wiki/Error_function#Approximation_with_elementary_functions">approximation</a> suitable for evaluation on the GPU:</p>

<pre><code>$$"erf"(x) ~~ 1 - 1/((1+a_1x + a_2x^2 + a_3x^3 + a_4x^4)^4)$$
</code></pre>

<p>where <code>$$a_1$$</code> = 0.278393, <code>$$a_2$$</code> = 0.230389, <code>$$a_3$$</code> = 0.000972, and <code>$$a_4$$</code> = 0.078108.</p>

<p>Now let&#8217;s finish simplifying <code>$$C(u,v)$$</code>:</p>

<pre><code>$$C_{"blurred box"}(u,v) ~~
    int_{y_{"min"}}^{y_{"max"}} G(y-v) int_{x_{"min"}}^{x_{"max"}} G(x-u) dxdy$$

$$= int_{y_{"min"}}^{y_{"max"}} G(y-v)
    (1/2 "erf"((x_{"max"}-u)/(sigma sqrt(2))) - 1/2 "erf"((x_{"min"}-u)/(sigma sqrt(2)))) dy$$

$$= 1/2 "erf"((x_{"max"}-u)/(sigma sqrt(2))) - 1/2 "erf"((x_{"min"}-u)/(sigma sqrt(2)))
    int_{y_{"min"}-v}^{y_{"max"}} G(y-v) dy$$

$$= 1/4 ("erf"((x_{"max"}-u)/(sigma sqrt(2))) - "erf"((x_{"min"}-u)/(sigma sqrt(2))))
        ("erf"((y_{"max"}-v)/(sigma sqrt(2))) - "erf"((y_{"min"}-v)/(sigma sqrt(2))))$$
</code></pre>

<p>And this gives us our closed form formula for the color of the blurred box.</p>

<p>Now for the real meat of the shader: the handling of nonzero border radii. CSS allows boxes to have <em>elliptical radii</em> in the corners, with separately defined major axis and minor axis lengths. Each corner can have separately defined radii; for simplicity, we only consider boxes with identical radii on all corners in this writeup, although the technique readily generalizes to heterogeneous radii. Most border radii on the Web are circular and homogeneous, but to handle CSS properly our shader needs to support elliptical heterogeneous radii in their full generality.</p>

<p>As before, the basic function to compute the pixel color looks like this:</p>

<pre><code>$$C(u,v) = sum_{y=-oo}^{oo} sum_{x=-oo}^{oo}G(x-u)G(y-v)C_{"rounded box"}(x,y)$$
</code></pre>

<p>where <code>$$C_{"rounded box"}(x,y)$$</code> is 1.0 if the point $$(x,y)$$ is inside the box (now with rounded corners) and 0.0 otherwise.</p>

<p>Adding some bounds to the sums gives us:</p>

<pre><code>$$C(u,v) = sum_{y=y_{"min"}}^{y_{"max"}} sum_{x=x_{"min"}}^{x_{"max"}} G(x-u)G(y-v)
    C_{"rounded box"}(x,y)$$
</code></pre>

<p><code>$$C_{"rounded box"}(x,y)$$</code> is 1.0 if <code>$$C_{"box"}(x,y)$$</code> is 1.0—i.e. if the point <code>$$(x,y)$$</code> is inside the unrounded box—<em>and</em> the point is either inside the ellipse defined by the value of the <code>border-radius</code> property or outside the border corners entirely. Let <code>$$C_{"inside corners"}(x,y)$$</code> be 1.0 if this latter condition holds and 0.0 otherwise—i.e. 1.0 if the point <code>$$(x,y)$$</code> is inside the ellipse defined by the corners or completely outside the corner area. Graphically, <code>$$C_{"inside corners"}(x,y)$$</code> looks like a blurry version of this:</p>

<div style="margin: 0 auto 1em; width: 300px; height: 200px; background: black; position: relative;">
    <div style="top: 25px; left: 25px; width: 250px; height: 150px; background: white; position: absolute;">
        <div style="position: absolute; top: 0; left: 0; right: 0; bottom: 0; background: black; border-radius: 25px;"></div>
    </div>
</div>


<p>Then, because <code>$$C_{"box"}(x,y)$$</code> is always 1.0 within the sum bounds, <code>$$C_{"rounded box"}(x,y)$$</code> reduces to <code>$$C_{"inside corners"}(x,y)$$</code>:</p>

<pre><code>$$C(u,v) = sum_{y=y_{"min"}}^{y_{"max"}} sum_{x=x_{"min"}}^{x_{"max"}} G(x-u)G(y-v)
    C_{"inside corners"}(x,y)$$
</code></pre>

<p>Now let <code>$$C_{"outside corners"}(x,y)$$</code> be the inverse of <code>$$C_{"inside corners"}(x,y)$$</code>—i.e. <code>$$C_{"outside corners"}(x,y) = 1.0 - C_{"inside corners"}(x,y)$$</code>. Intuitively, <code>$$C_{"outside corners"}(x,y)$$</code> is 1.0 if <code>$$(x,y)$$</code> is <em>inside</em> the box but <em>outside</em> the rounded corners—graphically, it looks like one <span style="display: inline-block; position: relative; width: 1em; height: 1em; overflow: hidden;"><span style="display: block; position: absolute; border-radius: 100%; width: 200%; height: 200%; top: -100%; left: -100%; border: solid black 1em;"></span></span> shape for each corner. With this, we can rearrange the formula above:</p>

<pre><code>$$C(u,v) = sum_{y=y_{"min"}}^{y_{"max"}} sum_{x=x_{"min"}}^{x_{"max"}} G(x-u)G(y-v)
    (1.0 - C_{"outside corners"}(x,y))$$

$$= sum_{y=y_{"min"}}^{y_{"max"}} sum_{x=x_{"min"}}^{x_{"max"}} G(x-u)G(y-v) -
    G(x-u)G(y-v)C_{"outside corners"}(x,y)$$

$$= (sum_{y=y_{"min"}}^{y_{"max"}} sum_{x=x_{"min"}}^{x_{"max"}} G(x-u)G(y-v)) -
    sum_{y=y_{"min"}}^{y_{"max"}} sum_{x=x_{"min"}}^{x_{"max"}}
        G(x-u)G(y-v)C_{"outside corners"}(x,y)$$

$$= C_{"blurred box"}(u,v) -
    sum_{y=y_{"min"}}^{y_{"max"}} sum_{x=x_{"min"}}^{x_{"max"}}
        G(x-u)G(y-v)C_{"outside corners"}(x,y)$$
</code></pre>

<p>We&#8217;ve now arrived at our basic strategy for handling border corners: compute the color of the blurred unrounded box, then &#8220;cut out&#8221; the blurred border corners by subtracting their color values. We already have a closed form formula for <code>$$C_{"blurred box"}(x,y)$$</code>, so let&#8217;s focus on the second term. We&#8217;ll call it <code>$$C_{"blurred outside corners"}(x,y)$$</code>:</p>

<pre><code>$$C_{"blurred outside corners"}(u,v) = sum_{y=y_{"min"}}^{y_{"max"}} sum_{x=x_{"min"}}^{x_{"max"}}
        G(x-u)G(y-v)C_{"outside corners"}(x,y)$$
</code></pre>

<p>Let&#8217;s subdivide <code>$$C_{"outside corners"}(x,y)$$</code> into the four corners: top left, top right, bottom right, and bottom left. This is valid because every point belongs to at most one of the corners per the CSS specification—corners cannot overlap.</p>

<pre><code>$$C_{"blurred outside corners"}(u,v) = sum_{y=y_{"min"}}^{y_{"max"}} sum_{x=x_{"min"}}^{x_{"max"}}
        G(x-u)G(y-v)(C_{"outside TL corner"}(x,y) + C_{"outside TR corner"}(x,y)
        + C_{"outside BR corner"}(x,y) + C_{"outside BL corner"}(x,y))$$

$$= (sum_{y=y_{"min"}}^{y_{"max"}} sum_{x=x_{"min"}}^{x_{"max"}}
        G(x-u)G(y-v)(C_{"outside TL corner"}(x,y) + C_{"outside TR corner"}(x,y))) +
        sum_{y=y_{"min"}}^{y_{"max"}} sum_{x=x_{"min"}}^{x_{"max"}}
            G(x-u)G(y-v)(C_{"outside BR corner"}(x,y) + C_{"outside BL corner"}(x,y))$$

$$= (sum_{y=y_{"min"}}^{y_{"max"}} G(y-v)
    ((sum_{x=x_{"min"}}^{x_{"max"}} G(x-u)C_{"outside TL corner"}(x,y)) +
    sum_{x=x_{"min"}}^{x_{"max"}} G(x-u) C_{"outside TR corner"}(x,y))) +
    sum_{y=y_{"min"}}^{y_{"max"}} G(y-v)
            ((sum_{x=x_{"min"}}^{x_{"max"}} G(x-u)C_{"outside BL corner"}(x,y)) +
            sum_{x=x_{"min"}}^{x_{"max"}} G(x-u)C_{"outside BR corner"}(x,y))$$
</code></pre>

<p>Let <code>$$a$$</code> and <code>$$b$$</code> be the horizontal and vertical border radii, respectively. The vertical boundaries of the top left and top right corners are defined by <code>$$y_min$$</code> on the top and <code>$$y_min + b$$</code> on the bottom; <code>$$C_{"outside TL corner"}(x,y)$$</code> and <code>$$C_{"outside TR corner"}(x,y)$$</code> will evaluate to 0 if <code>$$y$$</code> lies outside this range. Likewise, the vertical boundaries of the bottom left and bottom right corners are <code>$$y_max - b$$</code> and <code>$$y_max$$</code>.</p>

<p>(Note, again, that we assume all corners have equal border radii. The following simplification depends on this, but the overall approach doesn&#8217;t change.)</p>

<p>Armed with this simplification, we can adjust the vertical bounds of the sums in our formula:</p>

<pre><code>$$C_{"blurred outside corners"}(u,v) =
    (sum_{y=y_{"min"}}^{y_{"min"} + b} G(y-v)
        ((sum_{x=x_{"min"}}^{x_{"max"}} G(x-u)C_{"outside TL corner"}(x,y)) +
        sum_{x=x_{"min"}}^{x_{"max"}} G(x-u) C_{"outside TR corner"}(x,y))) +
    sum_{y=y_{"max"} - b}^{y_{"max"}} G(y-v)
            ((sum_{x=x_{"min"}}^{x_{"max"}} G(x-u)C_{"outside BL corner"}(x,y)) +
            sum_{x=x_{"min"}}^{x_{"max"}} G(x-u)C_{"outside BR corner"}(x,y))$$
</code></pre>

<p>And, following similar logic, we can adjust their horizontal bounds:</p>

<pre><code>$$C_{"blurred outside corners"}(u,v) =
    (sum_{y=y_{"min"}}^{y_{"min"} + b} G(y-v)
        ((sum_{x=x_{"min"}}^{x_{"min"} + a} G(x-u)C_{"outside TL corner"}(x,y)) +
        sum_{x=x_{"max"} - a}^{x_{"max"}} G(x-u) C_{"outside TR corner"}(x,y))) +
    sum_{y=y_{"max"} - b}^{y_{"max"}} G(y-v)
            ((sum_{x=x_{"min"}}^{x_{"min"} + a} G(x-u)C_{"outside BL corner"}(x,y)) +
            sum_{x=x_{"max"} - a}^{x_{"max"}} G(x-u)C_{"outside BR corner"}(x,y))$$
</code></pre>

<p>At this point, we can work on eliminating all of the <code>$$C_{"outside corner"}$$</code> functions from our expression. Let&#8217;s look at the definition of <code>$$C_{"outside TR corner"}(x,y)$$</code>. <code>$$C_{"outside TR corner"}(x,y)$$</code> is 1.0 if the point <code>$$(x,y)$$</code> is inside the rectangle formed by the border corner but outside the ellipse that defines that corner. That is, <code>$$C_{"outside TR corner"}(x,y)$$</code> is 1.0 if <code>$$y_{"min"} &lt;= y &lt;= y_{"min"} + b$$</code> and <code>$$E_{"TR"}(y) &lt;= x &lt;= x_{"max"}$$</code>, where <code>$$E_{"TR"}(y)$$</code> defines the horizontal position of the point on the ellipse with the given <code>$$y$$</code> coordinate. <code>$$E_{"TR"}(y)$$</code> can easily be derived from the equation of an ellipse centered at `$$(x_0, y_0)$$:</p>

<pre><code>$$(x-x_0)^2/a^2 + (y-y_0)^2/b^2 = 1$$

$$(x-x_0)^2 = a^2(1 - (y-y_0)^2/b^2)$$

$$x = x_0 + sqrt(a^2(1 - (y-y_0)^2/b^2))$$

$$E_{"TR"}(y) = x_0 + a sqrt(1 - ((y-y_0)/b)^2)$$
</code></pre>

<p>Parallel reasoning applies to the other corners.</p>

<p>Now that we have bounds within which each <code>$$C_{"outside corner"}$$</code> function evaluates to 1.0, we can eliminate all of these functions from the definition of <code>$$C_{"blurred outside corners"}$$</code>:</p>

<pre><code>$$C_{"blurred outside corners"}(u,v) =
    (sum_{y=y_{"min"}}^{y_{"min"} + b} G(y-v)
        ((sum_{x=x_{"min"}}^{E_{"TL"}(y)} G(x-u)) +
        sum_{x=E_{"TR"}(y)}^{x_{"max"}} G(x-u))) +
    sum_{y=y_{"max"} - b}^{y_{"max"}} G(y-v)
            ((sum_{x=x_{"min"}}^{E_{"BL"}(y)} G(x-u)) +
            sum_{x=E_{"BR"}(y)}^{x_{"max"}} G(x-u))$$
</code></pre>

<p>To simplify this a bit further, let&#8217;s define an intermediate function:</p>

<pre><code>$$E(y, y_0) = a sqrt(1 - ((y - y_0)/b)^2)$$
</code></pre>

<p>And rewrite <code>$$C_{"blurred outside corners"}(x,y)$$</code> as follows:</p>

<pre><code>$$C_{"blurred outside corners"}(u,v) =
    (sum_{y=y_{"min"}}^{y_{"min"} + b} G(y-v)
        ((sum_{x=x_{"min"}}^{x_{"min"} + a - E(y, y_{"min"} + b)} G(x-u)) +
        sum_{x=x_{"max"} - a + E(y, y_{"min"} + b)}^{x_{"max"}} G(x-u))) +
    (sum_{y=y_{"max" - b}}^{y_{"max"}} G(y-v)
        ((sum_{x=x_{"min"}}^{x_{"min"} + a - E(y, y_{"max"} - b)} G(x-u)) +
        sum_{x=x_{"max"} - a + E(y, y_{"max"} - b)}^{x_{"max"}} G(x-u)))$$
</code></pre>

<p>Now we simply follow the procedure we did before for the box. Approximate the inner sums with integrals:</p>

<pre><code>$$C_{"blurred outside corners"}(u,v) ~~
    (sum_{y=y_{"min"}}^{y_{"min"} + b} G(y-v)
        ((int_{x_{"min"}}^{x_{"min"} + a - E(y, y_{"min"} + b)} G(x-u)dx) +
        int_{x_{"max"} - a + E(y, y_{"min"} + b)}^{x_{"max"}} G(x-u)dx)) +
    (sum_{y=y_{"max" - b}}^{y_{"max"}} G(y-v)
        ((int_{x_{"min"}}^{x_{"min"} + a - E(y, y_{"max"} - b)} G(x-u)dx) +
        int_{x_{"max"} - a + E(y, y_{"max"} - b)}^{x_{"max"}} G(x-u)dx))$$
</code></pre>

<p>Replace <code>$$int G(x)dx$$</code> with its closed-form solution:</p>

<pre><code>$$C_{"blurred outside corners"}(u,v) ~~
    (sum_{y=y_{"min"}}^{y_{"min"} + b} G(y-v)
        (1/2 "erf"((x_{"min"} - u + a - E(y, y_{"min"} - v + b)) / (sigma sqrt(2))) -
         1/2 "erf"((x_{"min"} - u) / (sigma sqrt(2))) +
         (1/2 "erf"((x_{"max"} - u) / (sigma sqrt(2))) -
          1/2 "erf"((x_{"max"} - u - a + E(y, y_{"min"} - v + b)) / (sigma sqrt(2)))))) +
     sum_{y=y_{"max"} - b}^{y_{"max"}} G(y-v)
                (1/2 "erf"((x_{"min"} - u + a - E(y, y_{"max"} - v - b)) / (sigma sqrt(2))) -
                 1/2 "erf"((x_{"min"} - u) / (sigma sqrt(2))) +
                 (1/2 "erf"((x_{"max"} - u) / (sigma sqrt(2))) -
                  1/2 "erf"((x_{"max"} - u - a + E(y, y_{"max"} - v - b)) / (sigma sqrt(2)))))$$

$$= 1/2 (sum_{y=y_{"min"}}^{y_{"min"} + b} G(y-v)
        ("erf"((x_{"min"} - u + a - E(y, y_{"min"} - v + b)) / (sigma sqrt(2))) -
         "erf"((x_{"min"} - u) / (sigma sqrt(2))) +
         ("erf"((x_{"max"} - u) / (sigma sqrt(2))) -
          "erf"((x_{"max"} - u - a + E(y, y_{"min"} - v + b)) / (sigma sqrt(2)))))) +
     sum_{y=y_{"max"} - b}^{y_{"max"}} G(y-v)
                ("erf"((x_{"min"} - u + a - E(y, y_{"max"} - v - b)) / (sigma sqrt(2))) -
                 "erf"((x_{"min"} - u) / (sigma sqrt(2))) +
                 ("erf"((x_{"max"} - u) / (sigma sqrt(2))) -
                  "erf"((x_{"max"} - u - a + E(y, y_{"max"} - v - b)) / (sigma sqrt(2)))))$$
</code></pre>

<p>And we&#8217;re done! Unfortunately, this is as far as we can go with standard mathematical functions. Because the parameters to the error function depend on <code>$$y$$</code>, we have no choice but to evaluate the inner sum numerically. Still, this only results in <a href="https://github.com/glennw/webrender/blob/d57057470cb2bddf0c8ece3fc29cfbe5d03114a2/res/box_shadow.fs.glsl#L86">one loop in the shader</a>.</p>

<p>The <a href="https://github.com/glennw/webrender/blob/d57057470cb2bddf0c8ece3fc29cfbe5d03114a2/res/box_shadow.fs.glsl">current version of the shader</a> implements the algorithm basically as described here. There are several further improvements that could be made:</p>

<ol>
<li><p>The Gauss error function approximation that we use is accurate to <code>$$5 xx 10^-4$$</code>, which is way more accurate than we need. (Remember that the units here are 8-bit color values!) The approximation involves computing <code>$$x, x^2, x^3, " and " x^4$$</code>, which is expensive since we evaluate the error function many times for each pixel. It could be a nice speedup to replace this with a less accurate, faster approximation. Or we could use a lookup table.</p></li>
<li><p>We should not even compute the amount to subtract from the corners if the pixel in question is more than <code>$$3 sigma$$</code> pixels away from them.</p></li>
<li><p><code>$$C_{"blurred outside corners"}(x,y)$$</code> is a function of sigmoid shape. It might be interesting to try approximating it with a logistic function to avoid the loop in the shader. It might be possible to do this with a few iterations of least squares curve fitting on the CPU or with some sort of lookup table. Unfortunately, the parameters to the approximation will have to be per-box-shadow, because <code>$$C_{"blurred outside corners"}$$</code> depends on <code>$$a$$</code>, <code>$$b$$</code>, <code>$${x, y}_{"min, max"}$$</code>, and <code>$$sigma$$</code>.</p></li>
<li><p>Evaluating <code>$$G(x)$$</code> could also be done with a lookup table. There is already <a href="https://github.com/glennw/webrender/issues/66">a GitHub issue</a> filed on this.</p></li>
</ol>


<p>Finally, it would obviously be nice to perform some comprehensive benchmarks of this rendering algorithm, fully optimized, against the standard multiple-pass approach to drawing box shadows. In general, WebRender is not at all GPU bound on most hardware (like most accelerated GPU vector graphics rasterizers), so optimizing the count of GPU raster operations has not been a priority so far. When and if this changes (which I suspect it will as the rendering pipeline gets more and more optimized), it may be worth going back and optimizing the shader to reduce the load on the ALU. For now, however, this technique seems to perform quite well in basic testing, and since WebRender is so CPU bound it seems likely to me that the reduction in draw calls and state changes will make this technique worth the cost.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Revamped Parallel Layout in Servo]]></title>
    <link href="http://pcwalton.github.com/blog/2014/02/25/revamped-parallel-layout-in-servo/"/>
    <updated>2014-02-25T17:19:00-08:00</updated>
    <id>http://pcwalton.github.com/blog/2014/02/25/revamped-parallel-layout-in-servo</id>
    <content type="html"><![CDATA[<p>Over the past week I&#8217;ve submitted a <a href="https://github.com/mozilla/servo/pull/1700">series of</a> <a href="https://github.com/mozilla/servo/pull/1734">pull requests</a> that significantly revamp the way parallel layout works in Servo. Originally I did this work to improve performance, but it&#8217;s also turned out to be necessary to implement more advanced CSS 2.1 features. As it&#8217;s a fairly novel algorithm (as far as I&#8217;m aware) I&#8217;d like to take some time to explain it. I&#8217;ll start with where we are in Servo head and explain how it evolved into what&#8217;s in my branch. This post assumes a little knowledge about how browser engines work, but little else.</p>

<h2>Overview</h2>

<p>Servo&#8217;s layout operates on a <em>flow tree</em>, which is similar to the <em>render tree</em> in WebKit or Blink and the <em>frame tree</em> in Gecko. We call it a flow tree rather than a render tree because it consists of two separate data types: <em>flows</em>, which are organized in a tree, and <em>boxes</em>, which belong to flows and are organized in a flat list. Roughly speaking, a <em>flow</em> is an object that can be laid out in parallel with other flows, while a <em>box</em> is a box that must be laid out sequentially with other boxes in the same flow. If you&#8217;re familiar with WebKit, you can think of a box as a <code>RenderObject</code>, and if you&#8217;re familiar with Gecko, you can think of a box as a <code>nsFrame</code>. We want to lay out boxes in parallel as much as possible in Servo, so we group boxes into <em>flows</em> that can be laid out in parallel with one another.</p>

<p>Here&#8217;s a simple example. Suppose we have the following document:</p>

<pre><code>&lt;html&gt;
&lt;body&gt;
    &lt;p&gt;Four score and seven years ago our &lt;b&gt;fathers&lt;/b&gt; brought forth on this
    continent, a new nation, conceived in Liberty, and dedicated to the
    proposition that all men are created equal.&lt;/p&gt;
    &lt;p&gt;Now we are engaged in a great civil war, testing whether that nation,
    or any nation so conceived and so dedicated, can long endure. We are met
    on a great battle-field of that war. We have come to &lt;i&gt;dedicate&lt;/i&gt; a
    portion of that field, as a final resting place for those who here gave
    their lives that that nation might live. It is altogether fitting and
    proper that we should do this.&lt;/p&gt;
&lt;/body&gt;
&lt;/html&gt;
</code></pre>

<p>This would result in the following flow tree:</p>

<p><img src="http://i.imgur.com/uNZSiET.png" alt="(Flow tree)" /></p>

<p>Notice that there are three inline boxes under each <code>InlineFlow</code>. We have multiple boxes for each because each contiguous sequence of text in the same style—known as a <em>text run</em>—needs a box. During layout, the structure of the flow tree remains immutable, but the boxes get cut up into separate lines, so there will probably be many more boxes after layout (depending on the width of the window).</p>

<p>One neat thing about this two-level approach is that boxes end up flattened into a flat list instead of a linked data structure, improving cache locality and memory usage and making style recalculation faster because less needs to be allocated. Another benefit (and in fact the original impetus for this data structure) is that the line breaking code need not traverse trees in order to do its work—it only needs to traverse a single array, making the code simpler and improving cache locality.</p>

<p>Now that we know how the flow tree looks, let&#8217;s look at how Servo currently performs layout to figure out where boxes should go.</p>

<h2>The current algorithm</h2>

<p>The current algorithm for parallel layout in Servo (i.e. what&#8217;s in the master branch before my changes) consists of three separate passes over the flow tree.</p>

<ol>
<li><p><em>Intrinsic width calculation</em> or <code>bubble_widths</code> (bottom-up). This computes the <em>minimum width</em> and <em>preferred width</em> for each flow. There are no sequential hazards here and this can always be computed in parallel. Note that this is information is not always needed during layout, and eventually we will probably want to implement optimizations to avoid computation of this information for subtrees in which it is not needed.</p></li>
<li><p><em>Actual width calculation</em> or <code>assign_widths</code> (top-down). This computes the width of each flow, along with horizontal margins and padding values.</p></li>
<li><p><em>Height calculation</em> or <code>assign_heights</code> (bottom-up). This computes the height of each flow. Along the way, line breaking is performed, and floats are laid out. We also compute vertical margins and padding, including margin collapse.</p></li>
</ol>


<p>Within each flow, boxes are laid out sequentially—this is necessary because, in normal text, where to place the next line break depends on the lines before it. (However, we may be able to lay boxes out in parallel for <code>white-space: nowrap</code> or <code>white-space: pre</code>.)</p>

<p>For simple documents that consist of blocks and inline flows, Servo achieves excellent parallel wins, in line with <a href="http://www.eecs.berkeley.edu/~lmeyerov/projects/pbrowser/pubfiles/playout.pdf">Leo Meyerovich&#8217;s &#8220;Fast and Parallel Webpage Layout&#8221;</a>, which implemented this simple model.</p>

<h2>The problem with floats</h2>

<p>Unfortunately, in the real world there is one significant complication: floats. Here&#8217;s an example of a document involving floats that illustrates the problem:</p>

<pre><code>&lt;div style="float: right"&gt;
    I shot the sheriff.
    But I swear it was in self-defense.
    I shot the sheriff.
    And they say it is a capital offense.
&lt;/div&gt;
&lt;div&gt;
    I shot the sheriff
    But I didn't shoot no deputy.
&lt;/div&gt;
&lt;div&gt;
    All around in my home town,
    They're tryin' to track me down;
    They say they want to bring me in guilty
    For the killing of a deputy,
    For the life of a deputy.
&lt;/div&gt;
</code></pre>

<p>Rendered with a little extra formatting added, it looks like this:</p>

<div style='margin-left: auto; margin-right: auto; width: 300px; font: 14px "Times New Roman";'>
    <div style="float: right; width: 150px; white-space: pre-wrap; background-color: #0a0; color: white;">I shot the sheriff.
But I swear it was in self-defense.
I shot the sheriff.
And they say it is a capital offense.</div>
   <div style="white-space: pre-wrap">I shot the sheriff
But I didn&#8217;t shoot no deputy.</div>
    <div style="white-space: pre-wrap; margin-top: 12px;">All around in my home town,
They&#8217;re tryin&#8217; to track me down;
They say they want to bring me in guilty
For the killing of a deputy,
For the life of a deputy.</div>
</div>


<p>The flow tree for this document might look like this:</p>

<p><img src="http://i.imgur.com/s18ckTR.png" alt="(Flow tree)" /></p>

<p>Notice how the float in green (&#8220;I shot the sheriff…&#8221;) affects where the line breaks go in the two blocks to its left and below it (&#8220;I shot the sheriff…&#8221; and &#8220;All around…&#8221;). Line breaking is performed during the <em>height assignment</em> phase in Servo, because where the line breaks go determines the height of each flow.</p>

<p>This has important implications for the parallel algorithm described above. We don&#8217;t know how tall the float is until we&#8217;ve laid it out, and its height determines where to place the line breaks in the blocks next to it, so we have to lay out the float before laying out the blocks next to it. This means that we have to lay out the float before laying out any blocks that it&#8217;s adjacent to. But, more subtly, floats prevent us from laying out all the blocks that they impact in parallel as well. The reason is that we don&#8217;t know how many floats &#8220;stick out&#8221; of a block until we know its height, and in order to perform line breaking for a block we have to know how many floats &#8220;stuck out&#8221; of all the blocks before it. Consider the difference between the preceding document and this one:</p>

<div style='margin-left: auto; margin-right: auto; width: 300px; font: 14px "Times New Roman";'>
    <div style="float: right; width: 150px; white-space: pre-wrap; background-color: #0a0; color: white;">I shot the sheriff.
But I swear it was in self-defense.
I shot the sheriff.
And they say it is a capital offense.</div>
   <div style="white-space: pre-wrap">I shot the sheriff
But I didn&#8217;t shoot no deputy.
I shot the sheriff
But I didn&#8217;t shoot no deputy.</div>
    <div style="white-space: pre-wrap; margin-top: 12px;">All around in my home town,
They&#8217;re tryin&#8217; to track me down;
They say they want to bring me in guilty
For the killing of a deputy,
For the life of a deputy.</div>
</div>


<p>The only difference between the first document and the this one is that the first unfloated block (&#8220;I shot the sheriff…&#8221;) is taller. But this impacts the height of the second block (&#8220;All around…&#8221;), by affecting where the lines get broken. So the key thing to note here is that, in general, <em>floats force us to sequentialize the processing of the blocks next to them</em>.</p>

<p>The way this was implemented in Servo before my pull requests is that any floats in the document caused all unfloated blocks to be laid out sequentially. (The floats themselves could still be laid out in parallel, but all other blocks in the page were laid out in order.) Unsurprisingly, this caused our parallel gains to evaporate on most real-world Web pages. The vast majority of modern Web pages do use floats in some capacity, as they&#8217;re one of the most popular ways to create typical layouts. So losing our parallel gains is quite unfortunate.</p>

<p>Can we do better? It turns out we can.</p>

<h2>Clearing floats</h2>

<p>As most designers know, the <code>float</code> property comes with a very useful companion property: <code>clear</code>. The <code>clear</code> property causes blocks to be shifted down in order to avoid impacting floats in one or both directions. For example, the document above with <code>clear: right</code> added to the second block looks like this:</p>

<div style='margin-left: auto; margin-right: auto; width: 300px; font: 14px "Times New Roman";'>
    <div style="float: right; width: 150px; white-space: pre-wrap; background-color: #0a0; color: white;">I shot the sheriff.
But I swear it was in self-defense.
I shot the sheriff.
And they say it is a capital offense.</div>
   <div style="white-space: pre-wrap">I shot the sheriff
But I didn&#8217;t shoot no deputy.</div>
    <div style="clear: right; white-space: pre-wrap; margin-top: 12px;">All around in my home town,
They&#8217;re tryin&#8217; to track me down;
They say they want to bring me in guilty
For the killing of a deputy,
For the life of a deputy.</div>
</div>


<p>This property is widely used on the Web to control where floats can appear, and we can take advantage of this to gain back parallelism. If we know that no floats can impact a block due to the use of the <code>clear</code> property, then we can lay it out in parallel with the blocks before it. In the document above, the second block (&#8220;All around…&#8221;) can be laid out at the same time as the float and the first block.</p>

<p>My second pull request implements this optimization in this way: During flow construction, which is a bottom-up traversal, we keep track of a flag, <code>has_floated_descendants</code>, and set it on each flow if it or any of its descendants are <code>FloatFlow</code> instances. (Actually, there are two such flags—<code>has_left_floated_descendants</code> and <code>has_right_floated_descendants</code>—but for the purposes of this explanation I&#8217;ll just treat it as one flag.) During width computation, we iterate over our children and set two flags: <code>impacted_by_floats</code>. (Again, there are actually two such flags—<code>impacted_by_left_floats</code> and <code>impacted_by_right_floats</code>.) <code>impacted_by_floats</code> is true for a flow if and only if any of the following is true:</p>

<ol>
<li><p>The parent flow is impacted by floats.</p></li>
<li><p>The flow has floated descendants.</p></li>
<li><p>Any previous sibling flow is impacted by floats, <em>unless</em> the appropriate <code>clear</code> property has been set between this flow and that sibling.</p></li>
</ol>


<p>Only subtrees that have <code>impacted_by_floats</code> set to true are laid out sequentially, in order. The remaining subtrees can be laid out in parallel.</p>

<p>With this optimization implemented, documents above can be laid out in parallel as much as possible. It helps many real-world Web pages, as <code>clear</code> is a very commonly-used property.</p>

<p>At this point, two questions arise: &#8220;Can we do even more?&#8221; and &#8220;Is this algorithm enough to properly handle CSS?&#8221; As you might expect, the answer to the first is &#8220;yes&#8221;, and the answer to the second is &#8220;no&#8221;. To understand why, we need dive into the world of <em>block formatting contexts</em>.</p>

<h2>Block formatting contexts</h2>

<p>The behavior of <code>overflow: hidden</code> is subtle. Consider this document, which is identical to the document we&#8217;ve been using but with <code>overflow: hidden</code> specified on the blocks adjacent to the float:</p>

<pre><code>&lt;div style="float: right"&gt;
    I shot the sheriff.
    But I swear it was in self-defense.
    I shot the sheriff.
    And they say it is a capital offense.
&lt;/div&gt;
&lt;div style="overflow: hidden"&gt;
    I shot the sheriff
    But I didn't shoot no deputy.
&lt;/div&gt;
&lt;div style="overflow: hidden"&gt;
    All around in my home town,
    They're tryin' to track me down;
    They say they want to bring me in guilty
    For the killing of a deputy,
    For the life of a deputy.
&lt;/div&gt;
&lt;/div&gt;
</code></pre>

<p>Rendered, it looks like this:</p>

<div style='margin-left: auto; margin-right: auto; width: 300px; font: 14px "Times New Roman";'>
    <div style="float: right; width: 150px; white-space: pre-wrap; background-color: #0a0; color: white;">I shot the sheriff.
But I swear it was in self-defense.
I shot the sheriff.
And they say it is a capital offense.</div>
   <div style="overflow: hidden; white-space: pre-wrap">I shot the sheriff
But I didn&#8217;t shoot no deputy.</div>
    <div style="overflow: hidden; white-space: pre-wrap; margin-top: 12px;">All around in my home town,
They&#8217;re tryin&#8217; to track me down;
They say they want to bring me in guilty
For the killing of a deputy,
For the life of a deputy.</div></div>


<p></div></p>

<p>Notice that, with <code>overflow: hidden</code> specified, the float makes the entire width of the block next to it smaller: all the lines have been wrapped, not just those that impact the float.</p>

<p>What&#8217;s going on here is that <code>overflow: hidden</code> establishes what&#8217;s known as a <a href="https://developer.mozilla.org/en-US/docs/Web/Guide/CSS/Block_formatting_context">block formatting context</a> in CSS jargon. In Servo, block formatting contexts make our layout algorithm significantly more complex, because they require <em>width assignment and height assignment to be intertwined</em>, and for <em>height assignment to be interruptible</em>. To see why this is, recall that the flow tree for this document looks like this:</p>

<p><img src="http://i.imgur.com/s18ckTR.png" alt="(Flow tree)" /></p>

<p>Remember that Servo&#8217;s layout algorithm performs width calculation top-down, then height calculation bottom-up—this works under the assumption that widths never depend on heights. But with block formatting contexts adjacent to floats, this is no longer true: <em>the width of a block formatting context depends on the height of floats next to it</em>. This is because we don&#8217;t know whether a float, such as the green float above, is tall enough to impact a block formatting context, like those that the &#8220;I shot the sheriff…&#8221; and &#8220;All around…&#8221; above establish, until we lay out all blocks prior to the context and the float itself. And without knowing that, we cannot assign the width of the block formatting contexts.</p>

<p>To handle this case, my patches change Servo&#8217;s layout in several ways:</p>

<ol>
<li><p>When we see a block formatting context during width calculation, we check the value of the <code>impacted_by_floats</code> flag. If it is on, then we don&#8217;t calculate widths for that flow or any of its descendants. Instead, we set a flag called <code>width_assignment_delayed</code>.</p></li>
<li><p>When we encounter a block formatting context child of a flow while calculating heights, if that block formatting context has the flag <code>width_assignment_delayed</code> set, we <em>suspend</em> the calculation of heights for that node, calculate the width of the block formatting context, and begin calculating widths and heights for that node and all of its descendants (in parallel, if possible).</p></li>
<li><p>After calculating the height of a block formatting context, we <em>resume</em> calculation of heights for its parent.</p></li>
</ol>


<p>Let&#8217;s look at the precise series of steps that we&#8217;ll follow for the document above:</p>

<ol>
<li><p>Calculate the width of the root flow.</p></li>
<li><p>Calculate the width of the float flow.</p></li>
<li><p>Don&#8217;t calculate the widths of the two block flows; instead, set the <code>width_assignment_delayed</code> flag.</p></li>
<li><p>Calculate the width of the float flow&#8217;s inline flow child. The main width assignment phase is now complete.</p></li>
<li><p>Begin height calculation. First, calculate the height of the float flow and its inline child.</p></li>
<li><p>Start calculating the height of the root flow by placing the float.</p></li>
<li><p>We see that we&#8217;ve hit a block formatting context that has its width assignment delayed, so we clear that flag, determine its width, and start width calculation for its descendants.</p></li>
<li><p>Calculate width for the block flow&#8217;s inline child. Now width calculation is done for that subtree.</p></li>
<li><p>Calculate the height of the block flow&#8217;s inline child, and the block flow itself. Now height calculation is done for this subtree.</p></li>
<li><p>Resume calculating the height of the root flow. We see that the next block formatting context has its width assignment delayed, so we assign its width and repeat steps 8 and 9.</p></li>
<li><p>We&#8217;ve now calculated the height of the root flow, so we&#8217;re done.</p></li>
</ol>


<p>Now this particular page didn&#8217;t result in any parallel speedups. However, block formatting contexts can result in additional parallelism in some cases. For example, consider this document:</p>

<pre><code>&lt;div id=sidebar style="float: left"&gt;
    &lt;div&gt;Coupons&lt;/div&gt;
    &lt;div&gt;Freebies&lt;/div&gt;
    &lt;div&gt;Great Deals&lt;/div&gt;
&lt;/div&gt;
&lt;div id=main style="overflow: hidden"&gt;
    &lt;div&gt;Deals in your area:&lt;/div&gt;
    &lt;ul&gt;
    &lt;li&gt;Buy 1 lawyer, get 1 free&lt;/li&gt;
    &lt;li&gt;Free dental fillings&lt;/li&gt;
    &lt;/ul&gt;
&lt;/div&gt;
</code></pre>

<p>Rendered, it looks like this:</p>

<div style='margin-left: auto; margin-right: auto; width: 300px; font: 14px "Times New Roman";'>
    <div id=sidebar style="float: left; width: 75px; background-color: #0a0; color: white;">
        <div>Coupons</div>
        <div>Freebies</div>
        <div>Great Deals</div>
    </div>
    <div id=main style="overflow: hidden">
        <div>Deals in your area:</div>
        <ul>
        <li>Buy 1 lawyer, get 1 free</li>
        <li>Free dental fillings</li>
        </ul>
    </div>
</div>


<p>In this document, after we&#8217;ve laid out the sidebar, we can continue on and lay out the main part of the page entirely in parallel. We can lay out the block &#8220;Deals in your area&#8221; in parallel with the two list items &#8220;Buy 1…&#8221; and &#8220;Free dental fillings&#8221;. It turns out that this pattern is an extremely common way to create sidebars in real Web pages, so the ability to lay out the insides of block formatting contexts in parallel is a crucial optimization in practice. The upshot of all this is that block formatting contexts are a double-edged sword: they add an unfortunate dependency between heights and widths, but they enable us to recover parallelism even when blocks are impacted by floats, since we can lay out their interior in parallel.</p>

<h2>Conclusion</h2>

<p>No doubt about it, CSS 2.1 is tricky—floats perhaps more than anything else. But in spite of their difficulties, we&#8217;re finding that there are unexpected places where we can take advantage of parallelism to make a faster engine. I&#8217;m cautiously optimistic that Servo&#8217;s approaching the right design here—not only to make new content faster but also to accelerate the old.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Removing Garbage Collection From the Rust Language]]></title>
    <link href="http://pcwalton.github.com/blog/2013/06/02/removing-garbage-collection-from-the-rust-language/"/>
    <updated>2013-06-02T23:40:00-07:00</updated>
    <id>http://pcwalton.github.com/blog/2013/06/02/removing-garbage-collection-from-the-rust-language</id>
    <content type="html"><![CDATA[<p>I&#8217;ve been floating ways to simplify the memory management story in Rust around the core team lately. Memory management is a contentious topic, since we&#8217;ve worked hard to get to the current state of things, and with the push toward stability lately, there is a (quite reasonable!) resistance to any changes at this state. Still, I think the current memory management story in Rust is worth revisiting, as the current state of things may cause us problems down the line. Working with Dave Herman and Niko Matsakis, I&#8217;ve formulated a fairly concrete proposal at this point. The basic idea is to <em>remove garbage collection from the core language and relegate it to the standard library</em>, with a minimal set of language hooks in place to allow for flexible, pluggable automatic memory management.</p>

<p>This post is designed to explain the &#8220;why&#8221;, not the &#8220;how&#8221;—I&#8217;m leaving the concrete details of the proposed system to a future blog post or mailing list discussion. Rather, this explains the issue that I see with the current system. I think that the garbage collection story as it stands in Rust is not quite ideal, for three reasons: <em>familiarity</em>, <em>simplicity</em>, and <em>flexibility</em>. I&#8217;ll cover each in turn.</p>

<h1>Familiarity</h1>

<p>One of the most common questions almost every Rust beginner asks is &#8220;when do I use managed pointers, and when do I use owned pointers?&#8221; Or, more simply, &#8220;what are all these <code>~</code> and <code>@</code> symbols everywhere?&#8221; Having worked on Rust for many years now, I&#8217;ve seen several reasons for the difficulty. Chief among them are:</p>

<ol>
<li><p><em>The difference between the stack and the heap is a difficult concept to grasp for many programmers used to languages like Java that don&#8217;t make such a distinction.</em> This is, unfortunately, a fundamental difficulty of working in a systems language. There&#8217;s little that can be done about this without taking control of allocation out of the hands of the programmer. Doing that, however, would compromise the goals of the language—in low-level, performance-critical programming, being able to precisely control whether allocations occur on the stack or on the heap is crucial.</p></li>
<li><p><em>The sigils make the code unfamiliar before the concepts are learned.</em> Unlike the rest of the punctuation in Rust, <code>~</code> and <code>@</code> are not part of the standard repertoire of punctuation in C-like languages, and as a result the language can seem intimidating. One of the benefits of keywords is that they are self-documenting in a way that punctuation is not. This could be fixed by switching to keywords, which I prefer for this reason; however, syntactic beauty is in the eye of the beholder and so I won&#8217;t lose sleep over this not changing if the community prefers the current syntax.</p></li>
<li><p><em>There are two heaps, not just one, so beginners are confused as to which one to allocate into.</em> This is a result of the &#8220;minimize sharing by default&#8221; philosophy of the concurrency system. However, the concurrency system has been part of the <em>library</em> rather than the language for several years now, so this seems somewhat out of place.</p></li>
<li><p><em>Programmers don&#8217;t know which to use, since some operations are available with <code>~</code> and some operations are available with <code>@</code></em>. Actually, we were confused on this point for a long time as well—it wasn&#8217;t clear whether <code>~</code> or <code>@</code> would become dominant. We debated for a long time which to present first, <code>~</code> or <code>@</code>. However, as the language and community evolved, and coding standards became more settled, a clear winner emerged: the owning pointer <code>~</code>. In practice, the rule has been that <em>programmers should use <code>~</code> to allocate in all circumstances except when they have no way of knowing precisely when the object in question should be freed.</em></p></li>
</ol>


<p>Point (4), to me, is the most critical. The rule that emerged—<code>~</code> over <code>@</code>—should not be surprising, in retrospect, as it is how systems software has been developed for decades. The key insight that was missing is that <em>the owning pointer <code>~</code> is just the Rust equivalent of <code>malloc</code> and <code>free</code>.</em> For many, probably most C programs, <code>malloc</code> and <code>free</code> are just fine (assuming you use them correctly, of course); each heap allocation is allocated in just one place and destroyed in just one place. Only when the lifetimes of objects become very complex do C and C++ programmers resort to manual reference counting to determine when an object should be freed (and many, perhaps most, C programs never get there). <em>This</em> is the role that has emerged for <code>@</code> in Rust programs: <code>@</code> is a replacement for manual reference counting in C programs. The <code>kobject</code> system in the Linux kernel, the <code>GObject</code> system in <code>glib</code>, and so forth, are the C equivalents of <code>@</code> in Rust.</p>

<p>The key point here is that these are very specialized use cases in C, and <code>@</code> has been relegated to a similarly marginal role in idiomatic Rust code. We thought for a while that many Rust programs would use <code>@</code> extensively and that it would ease the learning curve for those not used to destructor-based memory management and references. This has not, however, been the case in practice. In reality, since the libraries all use owning pointers (<code>~</code>), Rust programmers have to learn them quickly anyhow. And once Rust programmers learn how to use <code>~</code> effectively, they quickly find <code>@</code> relegated to a marginal role, if it&#8217;s used at all. <code>~</code> has so many advantages: deterministic allocation and destruction, interaction with the standard library, freedom from GC marking pauses, simpler semantics, appendability where vectors and strings are concerned, and sendability across tasks.</p>

<p>I think we&#8217;re better off teaching <code>~</code> as the go-to solution for most programs and relegating <code>@</code> to a specialized role. <code>@</code> has its use cases, to be sure; large, event-driven C++ programs use reference counting for a reason. But those use cases are specialized. Beginners should not be asking &#8220;should I use <code>~</code> or <code>@</code>?&#8221; The answer is almost always <code>~</code>.</p>

<p>In this regard relegating <code>@</code> to a library is just the natural conclusion of this approach. I feel that what beginners should be taught is that <code>~</code> is the way to allocate in Rust, and letting an <code>~</code> owning pointer go out of scope is the way you free in Rust. This is what we should be teaching in the <em>language</em> tutorial. As beginners become more comfortable with this and explore the libraries, they will learn about ways to achieve more dynamic memory management: tracing garbage collection with the <code>Gc</code> type, reference counting with the <code>Rc</code> type, and thread-safe reference counting with the <code>Arc</code> type. But by building only <code>~</code> into the language, we can reduce confusion by, in effect, making the language more opinionated.</p>

<h1>Simplicity</h1>

<p>Although Rust didn&#8217;t start out that way, one of the most interesting applications of Rust has been very low-level programming, even down to the level of kernels. The interest in this application of Rust was something of a surprise to us, but in hindsight it makes perfect sense. Low-level control over memory management isn&#8217;t something that most applications software, especially on the server side, wants; most of that software has migrated over to languages like Java, Ruby, and JavaScript that trade control and performance for convenience by making memory management automatically, and dynamically, managed by the runtime. The remaining class of software, most of which is written in C and C++, is software that must manage memory manually in order to achieve some combination of performance, simplicity, and/or the ability to self-host. The prospect of using a new language for <em>this</em> class of software, which includes OS kernels, game engines, and browser engines among others, is what is fueling the growth of the nascent Rust community.</p>

<p>It might be possible to create a language that presents only a simple, fully automatic memory management system at first, and which surfaces the machinery of safe manual memory management* only when the programmer requires it for maximum performance. This would ease the learning curve, as programmers would be able to write many, perhaps most programs without ever learning how to manage memory at all. However, at this point I don&#8217;t think that this language exists yet, and in particular I don&#8217;t think Rust is that language. There are basically two problems here: (1) <code>~</code> owning pointers are everywhere in Rust, from the standard library to the built-in macros, making learning about them a necessity from the get-go; and (2) it is basically impossible to program Rust without at least a cursory understanding of references (a.k.a. <code>&amp;</code> pointers) and their lifetime semantics; even <code>vec::each()</code> uses references.</p>

<p>Despite the fact that this might seem like a negative result, I actually think it&#8217;s quite positive for the project. It helps to define the project&#8217;s scope. I don&#8217;t think automatic memory management in Rust is ever going to be as convenient as memory management in, say, Ruby or Java, and that&#8217;s OK! <em>The same level of control that adds cognitive overhead to memory management in Rust compared to other languages also makes Rust able to go where few other industry languages have.</em> This space, I think, is where Rust can really shine.</p>

<p>In short, I think that Rust as a <em>language</em> should focus on roughly the same application domain as C++ does.†</p>

<p>Important to this effort is to have as small of a runtime as possible, just as C++ does, leaving higher-level abstractions to libraries. And, in fact, we are almost there already. The only runtime support that compiled Rust programs require are a small set of &#8220;language items&#8221;, which are magic functions or traits written <em>in Rust</em> that are known to the compiler. Looking at the set of language items, and disqualifying legacy items that will be removed soon such as <code>annihilate</code> and <code>log_type</code>, there are just a few categories:</p>

<ol>
<li><p>Operator traits, like <code>Add</code> and <code>Sub</code>. These are analogous to <code>operator+</code>, <code>operator-</code>, and so forth in C++.</p></li>
<li><p>Memory primitives, like <code>str_eq</code>. These are somewhat legacy at this point and probably could be converted to LLVM intrinsics like <code>memcmp</code> without much trouble, especially after dynamically sized types happens. In any case, in most C++ compilers <code>memcmp</code> and friends are builtins.</p></li>
<li><p>Failure: <code>fail</code> and <code>fail_bounds_check</code>. This is analogous to <code>throw</code> in C++, although a Rust program that doesn&#8217;t want to use stack unwinding might want to use <code>abort</code> instead (which would be like <code>-fno-exceptions</code>) or do something more elaborate like the Linux kernel&#8217;s &#8220;oops&#8221; functionality.</p></li>
<li><p>Allocation primitives <code>malloc</code> and <code>free</code>. These have direct C++ equivalents: <code>operator new</code> and <code>operator delete</code>.</p></li>
<li><p>Garbage collection primitives.</p></li>
</ol>


<p>Of these, the only language items that don&#8217;t have direct C++ equivalents are the garbage collection primitives. If those were eliminated, then Rust as a language would be every bit as freestanding as C++ is. In terms of suitability for kernel and embedded development, Rust would be on truly equal footing.</p>

<p>In summary: (1) all Rust programmers have to know how <code>~</code> and <code>&amp;</code> work, despite the presence of <code>@</code>; (2) the only additional runtime primitives that Rust exposes and C++ doesn&#8217;t are those related to <code>@</code>.</p>

<h1>Flexibility</h1>

<p>When it comes to memory management, there are obviously many different strategies: stack allocation, heap allocation with <code>malloc</code> and <code>free</code>, arena allocation, and garbage collection. What&#8217;s less well known is that even among garbage collection, there are many different approaches, each with advantages and disadvantages. There&#8217;s thread-local GC, thread-safe GC, incremental GC, generational GC, reference counting, thread-safe reference counting, deferred reference counting, ulterior reference counting—the list goes on and on. (For a good survey of automatic memory management techniques and how they relate to one another, check out <a href="http://www.cs.virginia.edu/~cs415/reading/bacon-garbage.pdf">&#8220;A Unified Theory of Garbage Collection&#8221; by Bacon et al.</a>) A program that wants to maximize performance among some axis (latency versus throughput) and remain safe with objects with complex lifetimes may have reasons to choose one or the other.</p>

<p>Specifically, there&#8217;s the perennial debate between reference counting and tracing garbage collection. Many applications are better with tracing GC because of the increased throughput it provides and straightforward handling of cycles, and many applications are better with reference counting because of implementation simplicity, cache behavior, mostly-incremental operation, and promptness of deallocation. It makes sense for applications to be able to choose between the two. Even more important is the tradeoff between thread-safe and thread-local garbage collection: concurrent garbage collection is practically always more expensive than thread-local garbage collection, so it makes sense for programs to restrict concurrent GC (including atomic reference counting) to be used only when needed.</p>

<p>Integrating multiple tracing garbage collectors or cycle collectors into one system is a hard problem, and I don&#8217;t think Rust is going to really solve it. However, integrating reference counting into a garbage collected system is straightforward, as long as cycles are not created (and in Rust we can forbid the creation of such cycles through clever use of the type system). In practice this seems to work well: we typically use thread-local tracing GC for data with complex lifetimes within one task, and we use thread-safe reference counting for data that must be shared between tasks.</p>

<p>Equally important is the ability to integrate with <em>external</em> garbage collection systems (usually reference counted ones). This is a problem that is often overlooked, but is terribly important for client software such as mobile apps and browser engines. On Windows, apps must integrate with the reference-counted COM system in order to use DirectX and other APIs. On the Mac and on iOS, apps have to integrate with Objective-C and the closely-related Core Foundation, also reference-counted systems. On Linux, GNOME apps have to integrate with GObject, again a reference-counted system. On Android, apps have to integrate with the garbage-collected Dalvik subsystem via the JNI. All of this requires that the memory management system in the language be deeply flexible.</p>

<p>Because of this, I&#8217;m suspect of blessing any particular form of automatic memory management in the core language. In Rust, the <code>@</code> type is not only blessed with special syntax, but is eligible for borrowing and other operations in a way that user-defined types aren&#8217;t. Although Rust provides the facilities needed to build practically all the other forms of garbage collection, as well as those needed to integrate with external GC systems in a safe way, the resulting smart pointers feel second-class compared to <code>@</code>. A systems language designed to work in a diverse set of environments should have the flexibility to create memory management abstractions that feel first-class.</p>

<h1>Conclusion</h1>

<p>For these three reasons—familiarity, simplicity, and flexibility—I&#8217;d like to propose removing <code>@</code> pointers from the language and replacing them with a small set of hooks allowing the same functionality to be implemented as a library and on user-defined types. We would ship tracing GC as part of the standard library and make it just as powerful and convenient as it is today (except for the <code>@</code> syntax). We&#8217;d gain a flexible set of abstractions, make the language easier to learn, and make Rust into a truly freestanding language environment.</p>

<p>&#x2a; Note that the <em>safe</em> qualifier here disqualifies manually-built free lists in garbage-collected languages, as these manually-built free lists provide no protection against errors like double &#8220;frees&#8221;, leaks, and danging pointers. (They&#8217;re significantly worse than true manual memory management anyhow; the GC still has to trace through objects in arenas at mark time, copy the objects within out into the tenured generation when they survive a minor collection, write barrier the objects, and so forth.)</p>

<p>† Note that I don&#8217;t mean you shouldn&#8217;t write Web frameworks and Web sites in Rust: in fact, I think Rust would be a fantastic language for many classes of Web server software, especially that which must scale to the highest loads and squeeze every ounce of performance out of the servers on the racks.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Safe Manual Memory Management]]></title>
    <link href="http://pcwalton.github.com/blog/2013/05/20/safe-manual-memory-management/"/>
    <updated>2013-05-20T20:46:00-07:00</updated>
    <id>http://pcwalton.github.com/blog/2013/05/20/safe-manual-memory-management</id>
    <content type="html"><![CDATA[<p>If there&#8217;s one feature of Rust that is probably the most unique among languages in industry, it&#8217;s <em>safe manual memory management</em>.</p>

<p>It&#8217;s easiest to explain safe manual memory management by explaining how it differs from the memory models of other languages. There are a few different models in common use in industry languages:</p>

<ul>
<li><p><em>Unsafe manual memory management</em>—These languages provide very fine-grained control over memory allocation; heap memory can be explicitly allocated and deallocated. The most important examples here are C and C++. The well-known downside of this approach is that memory safety violations can only be detected at runtime with a memory safety checker such as Valgrind or Address Sanitizer. Memory safety violations that go unchecked can lead to crashes at best and exploitable security vulnerabilities at worst.</p></li>
<li><p><em>Full garbage collection</em>—The majority of modern languages expose a memory model that falls into this category—the space is very diverse, ranging from Java to Go to JavaScript to Ruby to Haskell. In general, such languages place all allocations into the heap instead of the stack, although escape analysis and value types may be used to reduce the number of heap allocations. Periodically, a <em>garbage collector</em> scans all pointers on the stack and in the heap, judges unreachable objects dead, and reclaims them. This approach has the advantage of <em>memory safety</em> at compile time—the language arranges for there to be no dangling pointers, wild pointers, and so forth. The downsides, however, are:</p>

<ol>
<li><p>The garbage collector may run at an inconvenient time. This can be mitigated by explicit control over when the GC runs, although if the garbage collector must collect multiple threads&#8217; heaps at the same time, this may be difficult to synchronize. This can also be mitigated by using manual memory pooling and free lists, although pooling has undesirable safety properties—much like unsafe manual memory management, there is no static guarantee that objects allocated from a pool are returned properly or that an object is not reachable when returned to the pool. Incremental and concurrent garbage collectors help here, but they are not free, as they typically require write and/or read barriers, reducing throughput.</p></li>
<li><p>When it runs, the garbage collector must mark all pointers to discover which ones are live, reducing throughput of the application. Essentially, the GC must discover at <em>runtime</em> what a C++ (say) programmer knows at <em>compile time</em>. Not much can typically be done about this cost in fully garbage-collected languages, short of falling back to unsafe manual memory management. Pools don&#8217;t help much here, because the GC must still trace the pointers into the pool. Even pointers into the stack generally must be traced.</p></li>
</ol>
</li>
<li><p><em>Garbage collection with value types and references</em>—This category includes languages like C#. (I believe D falls into this category as well, although I may be mistaken.) These languages are essentially garbage-collected, but they include <em>value types</em> which are guaranteed to be stack-allocated if in local variables. Additionally, and most importantly, they include <em>reference parameters</em> (and sometimes reference locals), which allow stack-allocated values to be temporarily aliased when calling another function. Effective use of value types can reduce marking and sweeping time. In general, this system is an effective addition to a garbage-collected system, allowing a good measure of additional control without much cost in complexity and no cost in memory safety. It is not, however, typically sufficient to write programs without using the garbage collector at all; the system is too simple to statically encode anything other than the most basic memory management patterns.</p></li>
</ul>


<p>Where does Rust fit in? Actually, it fits into a category all to itself among industry languages (although one shared by various research languages, like Cyclone). Rust offers <em>safe manual memory management</em> (although some have objected to the term &#8220;manual&#8221; here). It extends the system described above as &#8220;garbage collection with value types and references&#8221; in two important ways:</p>

<ol>
<li><p>You can allocate memory that will not be traced by the garbage collector, and free it manually if you choose. This is the feature of Rust known as &#8220;unique pointers&#8221;. Rust will automatically free memory that is uniquely owned when its owning pointer goes out of scope. It&#8217;s also easy to write a function that acts exactly as <code>free</code> does, so you can precisely choose when your objects die. Unique pointers are not traced by the GC (unless they point to a type that transitively contains a garbage-collected pointer), so they are an effective way to cut down marking times.</p></li>
<li><p>You can <em>return references</em> and <em>place references into data structures</em>. Like other references, these references are not traced by the garbage collector. As long as the references follow a <em>stack discipline</em>, meaning that they point to memory that was allocated by one of the callers of the current function, the compiler allows them to be placed anywhere. This adds a great deal of expressiveness over the reference parameter approach, and it enables a large number of programs to be written without using the garbage collector at all.</p></li>
</ol>


<p>In terms of safety and performance, safe manual memory management is having your cake and eating it too. You get memory safety like a garbage-collected language, but control like that of unsafe manual memory management. But this system has its downsides as well—most importantly, complexity of implementation and interface. Learning to use references and unique pointers poses a significant learning curve. But, once the system is learned, it&#8217;s remarkably flexible, with an attractive combination of performance and safety.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Performance of Sequential Rust Programs]]></title>
    <link href="http://pcwalton.github.com/blog/2013/04/18/performance-of-sequential-rust-programs/"/>
    <updated>2013-04-18T16:09:00-07:00</updated>
    <id>http://pcwalton.github.com/blog/2013/04/18/performance-of-sequential-rust-programs</id>
    <content type="html"><![CDATA[<p>Although Rust is designed for parallel programs, it is important that the performance of single-threaded, sequential programs not suffer in its design. As far as Servo is concerned, sequential performance is still important in many domains that a Web browser engine must compete in.</p>

<p>Below are some selected single-threaded benchmarks from the <a href="http://benchmarksgame.alioth.debian.org/">Computer Language Benchmarks Game</a> (formerly, and still informally, called the &#8220;shootout&#8221;). <em>This is far from an ideal set.</em> These benchmarks are showing their age quite heavily, they are too small and simplistic to extrapolate to real-world use cases, and many of them are too I/O-bound.</p>

<p>It is perfectly legal per the rules of the benchmarks game to use unsafe code (or calling libraries written in C, which is equivalent), and I believe it&#8217;s very difficult to precisely match C&#8217;s performance without resorting to unsafe code. (Practically speaking, one would need an extremely smart JIT, or a research language with a complex dependent type system.) As my colleague Niko pointed out, a more interesting benchmark would not allow <em>any</em> languages to use unsafe code and would exclude C and C++ from competing at all, except as a point of comparison—such a benchmark would be interesting to determine how much performance one has to trade for type safety in mainstream languages. But the shootout is what it is, and so the Rust versions of these benchmarks heavily use unsafe code. Over time, I hope to be able to reduce the amount of unsafe code present in the Rust versions of these benchmarks, but a couple of benchmarks will likely always remain unsafe.</p>

<p><em>Neither the C nor the Rust versions of these benchmarks use SIMD or threads.</em> This is by design, as the goal of this test is to measure Rust&#8217;s sequential performance. Over time, as Rust gains SIMD support and the scheduler improves (both of which are active areas of development), the benchmarks will be updated to use multiple threads. But keep in mind that <em>the C implementation tested against is not usually the top one on the shootout site; rather, I selected the fastest implementation that did not use SIMD or threads for comparison.</em> As the Rust benchmarks are updated to use SIMD and threads, equivalent C versions will be used for comparison.</p>

<p>For all these reasons and more, it is important to not read too much into these benchmark results. It would be a mistake to conclude that &#8220;Rust is faster than C&#8221; because of the performance on the <code>k-nucleotide</code> benchmark. Likewise, it would be a mistake to conclude that &#8220;C is faster than Rust&#8221; because of the <code>fasta-redux</code> benchmark. The goal here is simply to demonstrate that <em>sequential Rust can be written in a way that approaches competitive parity with equivalent C code.</em></p>

<p><em>Note that the benchmarks include <code>clang</code> because GCC 4.2 is a very old version. The purpose of this benchmark is not to benchmark C compilers, but rather to perform cross-implementation comparisons between two languages.</em></p>

<p>Enough disclaimers; on to the results:</p>

<p><img src="http://i.imgur.com/Cd3ZBHT.png" alt="Results" /></p>

<p>These programs were tested on a 2.53 GHz Intel Core 2 Duo MacBook Pro with 4 GB of RAM, running Mac OS X 10.6 Snow Leopard. &#8220;GCC 4.2&#8221; is GCC 4.2.1, Apple build 5666; &#8220;clang 1.7&#8221; is Apple clang 1.7, based on LLVM 2.9svn; &#8220;clang 3.1&#8221; is LLVM 3.1, trunk 149587. GCC and clang were run with <code>-O2</code>, and Rust was run with <code>-O</code> (which is like <code>-O2</code>). Three runs were averaged together to produce each result. Results are normalized to GCC 4.2. Lower numbers are better.</p>

<p>As mentioned before, this is a selected set of benchmarks. The benchmarks that were not included are:</p>

<ul>
<li><p><code>fasta</code> is omitted because it is similar to <code>fasta-redux</code>.</p></li>
<li><p><code>regexp-dna</code> is omitted because it consists of an uninteresting binding to PCRE.</p></li>
<li><p><code>binary-trees</code> is omitted because it is a garbage collection benchmark and the C version uses an arena, defeating the purpose (although I suspect a Rust version that did the same would do well).</p></li>
<li><p><code>chameneos-redux</code> and <code>threadring</code> are omitted because they are threading benchmarks.</p></li>
</ul>


<p>You can see the changes to the Rust compiler that were made to optimize these tests, as well as the benchmark sources, on my <a href="https://github.com/pcwalton/rust/tree/shootout">branch</a> of the compiler on GitHub. The goal will be to land these changes over the next few days.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[A Hard Case for Memory Safety]]></title>
    <link href="http://pcwalton.github.com/blog/2013/04/12/a-hard-case-for-memory-safety/"/>
    <updated>2013-04-12T14:01:00-07:00</updated>
    <id>http://pcwalton.github.com/blog/2013/04/12/a-hard-case-for-memory-safety</id>
    <content type="html"><![CDATA[<p>Quick quiz: In this C++ program, is the definition of <code>munge</code> guaranteed to be memory safe? (Assume that the definition of <code>increment_counter</code> uses only modern C++ idioms and doesn&#8217;t do anything like dereference an invalid pointer.)</p>

<pre><code>#include &lt;iostream&gt;
#include &lt;vector&gt;

class foo {
public:
    std::vector&lt;int&gt; indices;
    int counter;

    foo() : indices(), counter(0) {
        indices.push_back(1);
        indices.push_back(2);
        indices.push_back(3);
    }

    void increment_counter();

    int &amp;get_first_index() {
        assert(indices.size() &gt; 0);
        return indices[0];
    }

    void munge() {
        int &amp;first = get_first_index();
        increment_counter();
        std::cout &lt;&lt; first &lt;&lt; std::endl;
        first = 20;
    }
};

int main() {
    foo foo;
    foo.munge();
    return 0;
}
</code></pre>

<p>The answer: Even with this caveat, we can&#8217;t tell! It depends on the definition of <code>increment_counter</code>.</p>

<p>If <code>increment_counter</code> has this definition, the code is memory safe:</p>

<pre><code>void foo::increment_counter() {
    counter++;
}
</code></pre>

<p>But if <code>increment_counter</code> has this definition, for example, then it isn&#8217;t:</p>

<pre><code>void foo::increment_counter() {
    indices.clear();
    counter++;
}
</code></pre>

<p>This definition would cause the <code>first</code> reference in <code>munge</code> to become a dangling reference, and the call to <code>std::cout</code> and subsequent assignment of <code>first</code> will have undefined behavior. If <code>first</code> were not an <code>int</code> but were instead an instance of a class, and <code>munge</code> attempted to perform a virtual method call on it, then this would constitute a critical security vulnerability.</p>

<p>The point here is that determining memory safety in C++ requires <em>non-local</em> reasoning. Any analysis that tries to determine safety of C++ code, whether performed by a machine or performed by a human auditor, has to analyze many functions all at once, rather than one function at a time, to determine whether the code is memory safe. As this example illustrates, sticking to modern C++ coding styles, even with bounds checks, is not enough to prevent this.</p>

<p>There are a few ways around this:</p>

<ul>
<li><p>For each function call, analyze the source to the called function to determine whether it&#8217;s memory safe <em>in the context of the caller</em>. This doesn&#8217;t always work, though: it&#8217;s hard or impossible when function pointers or virtual methods are involved (which function ends up being called?), and it&#8217;s hard with separately compiled code (what if the called function is in a DLL that you don&#8217;t have source for?)</p></li>
<li><p>Change the type of <code>indices</code> to <code>std::vector&lt;std::shared_ptr&lt;int&gt;&gt;</code>; i.e. use reference counting to keep the pointer alive. This has a runtime cost.</p></li>
<li><p>Inline the body of <code>increment_counter</code>, so that the memory safety of <code>munge</code> is immediately clear.</p></li>
<li><p>Make <code>increment_counter</code> a class method (or just a function) instead of an instance method, and have it take <code>counter</code> by reference. The idea here is to prevent the possibility that <code>increment_counter</code> could mess with <code>indices</code> in any way by shutting off its access to it.</p></li>
</ul>


<p>What does this have to do with Rust? In fact, this error corresponds to a borrow check error that Brian Anderson hit when working on the scheduler. In Rust, the corresponding code looks something like this:</p>

<pre><code>impl Foo {
    fn get_first_index(&amp;'a mut self) -&gt; &amp;'a mut int {
        assert!(self.indices.len() &gt; 0);
        return &amp;mut indices[0];
    }

    fn munge(&amp;mut self) {
        let first = self.get_first_index();
        self.increment_counter(); // ERROR
        println(first.to_str());
        *first = 20;
    }
}
</code></pre>

<p>This causes a borrow check error because the <code>first</code> reference conflicts with the call to <code>increment_counter</code>. The reason the borrow check complains is that the borrow check only checks one function at a time, and it could tell (quite rightly!) that the call to <code>increment_counter</code> might be unsafe. The solution is to make <code>increment_counter</code> a static method that only has access to counter; i.e. to rewrite the <code>self.increment_counter()</code> line as follows:</p>

<pre><code>Foo::increment_counter(&amp;mut self.counter);
</code></pre>

<p>Since the borrow check now sees that <code>increment_counter</code> couldn&#8217;t possibly destroy the <code>first</code> reference, it now accepts the code.</p>

<p>Fortunately, such borrow check errors are not as common anymore, with the new simpler borrow check rules. But it&#8217;s interesting to see that, when they do come up, they&#8217;re warning about real problems that affect any language with manual memory management. In the C++ code above, most programmers probably wouldn&#8217;t notice the fact that the memory safety of <code>munge</code> depends on the definition of <code>increment_counter</code>. The challenge in Rust, then, will be to make the error messages comprehensible enough to allow programmers to understand what the borrow checker is warning about and how to fix any problems that arise.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[An Overview of Memory Management in Rust]]></title>
    <link href="http://pcwalton.github.com/blog/2013/03/18/an-overview-of-memory-management-in-rust/"/>
    <updated>2013-03-18T15:07:00-07:00</updated>
    <id>http://pcwalton.github.com/blog/2013/03/18/an-overview-of-memory-management-in-rust</id>
    <content type="html"><![CDATA[<p>One of the key features of Rust that sets it apart from other new languages is that its memory management is <em>manual</em>—the programmer has explicit control over where and how memory is allocated and deallocated. In this regard, Rust is much more like C++ than like Java, Python, or Go, to name a few. This is an important design decision that makes Rust able to function in performance-critical domains that safe languages previously haven&#8217;t been able to—top-of-the line games and Web browsers, for example—but it adds a nontrivial learning curve to the language.</p>

<p>For programmers familiar with modern C++, this learning curve is much shallower, but for those who are used to other languages, Rust&#8217;s smart pointers can seem confusing and complex. In keeping with the systems-oriented nature of Rust, this post is designed to explain how Rust&#8217;s memory management works and how to effectively use it.</p>

<h2>Smart pointers</h2>

<p>In many languages with manual memory management, like C, you directly allocate and free memory with calls to special functions. For example:</p>

<pre><code>void f() {
    int *x = malloc(sizeof(int));  /* allocates space for an int on the heap */
    *x = 1024;                     /* initialize the value */
    printf("%d\n", *x);            /* print it on the screen */
    free(x);                       /* free the memory, returning it to the heap */
}
</code></pre>

<p>C gives you a great deal of control over where memory is allocated and deallocated. Memory is allocated with a special function <code>malloc</code>, and it is freed with a special function <code>free</code>. After the call to <code>free</code>, it is an error to attempt to use <code>x</code>, as it is a <em>dangling pointer</em>. A dangling pointer points to invalid memory, but the C compiler makes no attempt to prevent you from using it; it&#8217;s your responsibility to avoid touching it after freeing the memory it points to.</p>

<p>Rust gives you the same level of control over memory, but it works somewhat differently. Let&#8217;s see how the same piece of code looks in Rust:</p>

<pre><code>fn f() {
    let x: ~int = ~1024;          // allocate space and initialize an int
                                  // on the heap
    println(fmt!("%d", *x));      // print it on the screen
} // &lt;-- the memory that x pointed at is automatically freed here
</code></pre>

<p>There are three main differences to notice here:</p>

<ol>
<li><p>In C, you allocate memory first (with the call to <code>malloc</code>), and then you initialize it (in the example above, with the <code>*x = 1024</code> assignment). Rust fuses the two operations together into the <code>~</code> allocation operator, so that you don&#8217;t accidentally forget to initialize memory before you use it.</p></li>
<li><p>In C, the call to <code>malloc</code> returns a plain pointer, <code>int *</code>. In Rust, the <code>~</code> operator, which allocates memory, returns a special <em>smart pointer</em> to an int. Because this type of smart pointer is so common, its name is just a single character, <code>~</code>—thus the type of this smart pointer is written as <code>~int</code>.</p></li>
<li><p>You don&#8217;t call <code>free</code> manually in Rust. Rather, the compiler automatically frees the memory for you when a smart pointer goes out of scope.</p></li>
</ol>


<p>As it turns out, points (2) and (3) are very intertwined, and together they form the cornerstone of Rust&#8217;s memory management system. Here&#8217;s the idea: Unlike C, allocation functions in Rust don&#8217;t return a raw pointer to the space they allocate. Instead, they return a <em>smart pointer</em> to the space. A smart pointer is a special kind of value that controls when the object is freed. Like a raw pointer in C, you can access the data that a smart pointer refers to with <code>*</code>. But unlike a raw pointer, <em>when the smart pointer to an allocation goes out of scope, that allocation is automatically freed.</em> In this way, smart pointers are &#8220;smart&#8221; because they not only track where an object is but also track how to clean it up.</p>

<p>Unlike C, in Rust you never call <code>free</code> directly. Instead, you rely on smart pointers to free all allocations. The most basic reason for this is that smart pointers make it harder to forget to free memory. In C, if you forget to call <code>free</code>, you have a <em>memory leak</em>, which means that the memory will not be cleaned up until the program exits. However, in Rust, the compiler will automatically insert the code necessary to free the memory for you when the smart pointer pointing to your data goes out of scope.</p>

<p>Rust has multiple types of smart pointers, corresponding to the different strategies that programs use to reclaim memory. Some smart pointers, namely <code>~</code> and <code>@</code> (which we will cover shortly), have special names known to the compiler, because they&#8217;re so common. (Having to type long names like <code>unique_ptr</code> all the time would be a burden.) Other smart pointers, such as <code>ARC</code> (which allows you to share read-only data between threads), are in the standard library and are not built into the compiler.</p>

<p>The pointer covered above is known as the <em>unique smart pointer</em> <code>~</code>. We call it &#8220;unique&#8221; because there is always only one smart pointer pointing to each allocation. The other type of smart pointer built into the language is the <em>managed smart pointer</em>, which allows <em>multiple</em> smart pointers to point to the same allocation and uses <em>garbage collection</em> to determine when to free it. Here&#8217;s an example of a managed smart pointer in use:</p>

<pre><code>fn foo() {
    let x: @int = @1024;     // allocate space and initialize an int
                             // on the heap
    bar(x);                  // pass it to `bar`
    println(fmt!("%d", *x)); // print it on the screen
} // &lt;-- the memory can be freed here

fn bar(x: @int) {
    let y: @int = x;         // make a new smart pointer to `x`
} // &lt;-- despite `y` going out of scope, the memory is *not* freed here
</code></pre>

<p>The key difference between <code>~</code> and <code>@</code> is that <code>@</code> allows <em>multiple</em> smart pointers to point to the same data, and the data is cleaned up only after the <em>last</em> such smart pointer disappears. Notice that, in this example, the memory pointed at by <code>y</code> (which is the same as the memory pointed at by <code>x</code>) is not freed at the end of the function <code>bar</code>, because <code>x</code> is still in use and also points to the same data. The fact that <code>@</code> allows multiple smart pointers to the same data, as well as the fact that the allocation is freed only when all of those pointers go out of scope, make managed smart pointers very useful. However, they can be less efficient than unique smart pointers, as they require garbage collection at runtime.</p>

<h2>References</h2>

<p>Recall that a smart pointer is a pointer that automatically frees the memory that it points to when it goes out of scope. Perhaps surprisingly, it often turns out that it&#8217;s useful to have a kind of pointer that <em>doesn&#8217;t</em> free the memory that it points to. Consider this code:</p>

<pre><code>struct Dog {
    name: ~str    // a unique smart pointer to a string
}

fn dogshow() {
    let dogs: [~Dog * 3] = [        // create an array of Dog objects
        ~Dog { name: ~"Spot"   },   // use unique smart pointers to
                                    // allocate
        ~Dog { name: ~"Fido"   },
        ~Dog { name: ~"Snoopy" },
    ];
    for dogs.each |dog| {
        println(fmt!("Say hello to %s", dog.name));
    }
} // &lt;-- all dogs destroyed here
</code></pre>

<p>Suppose that we wanted to single Fido out as the winner of the dog show. We might try this code:</p>

<pre><code>fn dogshow() {
    let dogs: [~Dog * 3] = [
        ~Dog { name: ~"Spot"   },
        ~Dog { name: ~"Fido"   },
        ~Dog { name: ~"Snoopy" },
    ];
    let winner: ~Dog = dogs[1];
    for dogs.each |dog| {
        println(fmt!("Say hello to %s", dog.name));
    }
    println(fmt!("And the winner is: %s!", winner.name));
} // &lt;-- all dogs, and `winner`, destroyed here
</code></pre>

<p>But this code won&#8217;t compile. The reason is that, if it did, Fido would be destroyed twice. Remember that <em>unique smart pointers free the allocations they point to when they go out of scope</em>. The code attempts to make a second smart pointer to Fido at the time it executes the line <code>let winner: ~Dog = dogs[1];</code> If the compiler allowed this to proceed, then at the end of the block, the program would attempt to free Fido twice—once when it frees the original smart pointer embedded within the <code>dogs</code> array, and once when it frees <code>winner</code>.</p>

<p>What we really want is for <code>winner</code> to be a pointer that <em>doesn&#8217;t</em> free the allocation that it points to. In fact, what we want isn&#8217;t a smart pointer at all; we want a <em>reference</em>. Here&#8217;s the code rewritten to use one:</p>

<pre><code>fn dogshow() {
    let dogs: [~Dog * 3] = [
        ~Dog { name: ~"Spot"   },
        ~Dog { name: ~"Fido"   },
        ~Dog { name: ~"Snoopy" },
    ];
    let winner: &amp;Dog = dogs[1];  // note use of `&amp;` to form a reference
    for dogs.each |dog| {
        println(fmt!("Say hello to %s", dog.name));
    }
    println(fmt!("And the winner is: %s!", winner.name));
} // &lt;-- all dogs destroyed here
</code></pre>

<p>This code will now compile. Here, we convert <code>winner</code> into a reference, notated in Rust with <code>&amp;</code>. You can take a reference to any smart pointer type in Rust by simply assigning it to a value with a reference type, as the <code>let winner: &amp;Dog = dogs[1]</code> line does.</p>

<p>References (also known as <em>borrowed pointers</em>) don&#8217;t cause the compiler to free the data they refer to. However, they don&#8217;t <em>prevent</em> the compiler from freeing anything either. They have no effect on what smart pointers will do; regardless of how many references you have, a unique smart pointer will always free the data that it points to when it goes out of scope, and a managed smart pointer will always free its data when all managed smart pointers to the same allocation go out of scope.</p>

<p>This is important to keep in mind. Code like this will not compile:</p>

<pre><code>fn foo() {
    let y: &amp;int;
    {
        let x: ~int = ~2048;
        y = x;
    } // &lt;-- x freed here
    println(fmt!("Your lucky number is: %d", *y)); // ERROR: accesses freed data!
}
</code></pre>

<p>In languages like C++, code like this could cause faults from attempting to access invalid memory. As it turns out, however, this piece of code won&#8217;t compile—the Rust compiler can and does prevent you from writing code like this at compile time. Essentially, the Rust compiler <em>tracks where each reference came from</em> and reports an error if a reference persists longer than the allocation it points into. This means that, generally speaking, you can use references all you like and have the confidence that they won&#8217;t result in hard-to-diagnose errors at runtime.</p>

<h2>Conclusion</h2>

<p>These ideas—smart pointers and references—form the basis of memory management in Rust. If you&#8217;re a C++ programmer, most of this will (hopefully!) simply have been an exercise in learning different syntax. For other programmers, these concepts are likely more foreign. But using these tools, you can write code with fine-grained control over memory, with improved safety over languages like C.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Which Pointer Should I Use?]]></title>
    <link href="http://pcwalton.github.com/blog/2013/03/09/which-pointer-should-i-use/"/>
    <updated>2013-03-09T12:05:00-08:00</updated>
    <id>http://pcwalton.github.com/blog/2013/03/09/which-pointer-should-i-use</id>
    <content type="html"><![CDATA[<p>Deciding whether to use a managed <code>@</code> pointer or an owned <code>~</code> pointer to allocate memory is one of the most frequent sources of confusion for newcomers to Rust. There are two main angles to consider when deciding whether to use an <code>@</code> pointer or a <code>~</code> pointer in Rust: <em>memory management</em> and <em>concurrency</em>. I&#8217;ll cover each in turn.</p>

<p>Note that this tutorial only presents the basic system. There are many extensions to the system—borrowing, library smart pointers, cells, and so on—that allow the various limitations described here to be overcome. But this is the core system that needs to be understood first.</p>

<h1>Memory management</h1>

<p>One of the most important features of Rust from a systems programming perspective is that garbage collection is optional. What this means is that there are safe ways to allocate memory that do not require bookkeeping at runtime to determine when it is safe to free that memory.</p>

<p>What makes it possible for Rust programs to avoid runtime garbage collection is the notion of <em>ownership</em> of a particular allocation. Under this scheme, when the single owner of an allocation goes out of scope, the allocation is freed. Owned pointers in Rust are notated with <code>~</code>. Here&#8217;s an example of their use:</p>

<pre><code>struct Point {
    x: int,
    y: int,
}

fn f() {
    let x: ~Point = ~Point { x: 10, y: 20 };  // allocate a Point on the heap
}  // &lt;-- x is freed here
</code></pre>

<p>Here, <code>x</code> is the single owner of the <code>Point</code> on the heap. Because there is only a single owner, Rust can throw away the memory pointed to by <code>x</code> at the end of the function.</p>

<p>The compiler enforces that there is only a single owner. Assigning the pointer to a new location <em>transfers ownership</em> (known as a <em>move</em> for short). Consider this program:</p>

<pre><code>fn g() {
    let a: ~Point = ~Point { x: 10, y: 20 }; // allocate a Point on the heap
    let b = a;                               // now b is the owner
    println(b.x.to_str());                   // OK
    println(a.x.to_str());                   // ERROR: use of moved value
} // &lt;-- b is freed here
</code></pre>

<p>When compiling this program, the compiler produces the error &#8220;use of moved value&#8221;. This is because assigning an owned pointer transfers ownership, making the old variable <em>dead</em>. Because the compiler knows precisely which variables are dead at all times, it can avoid having to determine at runtime whether to free the memory that a variable points to, and it can prevent you from accidentally accessing dead variables. However, this comes at a price: you are limited to using a single variable to refer to an <code>~</code> allocation.</p>

<p>By contrast, <code>@</code> pointers do not have this limitation. We think of memory that is allocated with <code>@</code> as <em>owned by the garbage collector</em>. You can make as many pointers to <code>@</code> memory as you would like. There is a cost in runtime performance, but this cost comes with a great deal of flexibility. For example, the code above will compile with an <code>@</code> pointer:</p>

<pre><code>fn h() {
    let a: @Point = @Point { x: 10, y: 20 }; // allocate a Point on the heap
    let b = a;                               // a and b share a reference
    println(b.x.to_str());                   // OK
    println(a.x.to_str());                   // also OK
}
</code></pre>

<p>So, in short: <em><code>@</code> pointers require garbage collection, but allow multiple pointers to the same location. <code>~</code> pointers avoid this GC overhead, but they don&#8217;t allow multiple pointers to the same location.</em></p>

<h1>Concurrency</h1>

<p>Another equally important aspect to the distinction between <code>@</code> and <code>~</code> is that it ensures that concurrent Rust tasks don&#8217;t race on shared memory. To illustrate this, here&#8217;s an example of broken code that doesn&#8217;t compile:</p>

<pre><code>struct Counter {
    count: int
}

fn f() {
    // Allocate a mutable counter.
    let counter: @mut Counter = @mut Counter { count: 0 };
    do spawn {               // spawn a new thread
        // Increment the counter.
        counter.count += 1;  // ERROR: attempt to capture an `@` value
    }
    println(counter.count.to_str()); // print the value
}
</code></pre>

<p>This code contains a classic <em>race</em>—if this code compiled, then the value printed would be either 0 or 1, depending on whether the <code>counter.count += 1</code> line executed first or the <code>println</code> executed first. The key here is that two threads—the spawned thread and the main thread—are both simultaneously attempting to access the <code>counter</code> object. To prevent these errors, Rust prevents multiple threads from accessing the same memory at the same time.</p>

<p>Recall from the previous section that there can be any number of pointers to memory allocated with <code>@</code>. But there can be only one pointer to memory allocated with <code>~</code>. This suggests a way to forbid multiple threads from accessing the same data: <em>restrict the types of pointers that can be sent between threads to <code>~</code> pointers</em>. And this is exactly what Rust does.</p>

<p>For any piece of <code>~</code>-allocated memory, there is only one pointer to it, and that pointer is owned by exactly one thread. So there can be no races, since any other threads simply don&#8217;t have access to that memory. Let&#8217;s rewrite our example above using <code>~</code> to illustrate this:</p>

<pre><code>fn g() {
    // Allocate a mutable counter.
    let mut counter: ~Counter = ~Counter { count: 0 };
    do spawn {               // spawn a new thread
        counter.count += 1;  // increment the counter
    }
    println(counter.count.to_str()); // ERROR: use of moved value
}
</code></pre>

<p>What&#8217;s going on here is that, by referring to <code>counter</code> inside the <code>spawn</code> block, the new thread <em>takes ownership</em> of the <code>counter</code> variable, and the <code>counter</code> variable becomes dead everywhere outside that block. Essentially, the main thread loses access to <code>counter</code> by <em>giving it away</em> to the thread it spawns. So the attempt to print the value on the screen from the main thread will fail. By contrast, this code will work:</p>

<pre><code>fn h() {
    // Allocate a mutable counter.
    let mut counter: ~Counter = ~Counter { count: 0 };
    do spawn {               // spawn a new thread
        counter.count += 1;  // increment the counter
        println(counter.count.to_str()); // OK: `counter` is owned by this thread
    }
}
</code></pre>

<p>Notice that the data race is gone: this code always prints <code>1</code>, because the printing happens in the thread that owns the <code>Counter</code> object.</p>

<p>The resulting rule is pretty simple. In short: <em><code>@</code> pointers may not be sent from thread to thread. <code>~</code> pointers may be sent, and are owned by exactly one thread at a time.</em> Therefore, if you need data to be sent, do not allocate it with <code>@</code>.</p>

<h1>Conclusion (TL;DR)</h1>

<p>So the distinction between <code>@</code> and <code>~</code> is often confusing to newcomers, but it&#8217;s really quite simple. There are two main rules to remember:</p>

<ol>
<li><p><code>~</code> only supports one pointer to each allocation, so if you need multiple pointers to the same data, use <code>@</code>. But <code>@</code> requires garbage collection overhead, so if this is important to your application, use <code>~</code> wherever possible.</p></li>
<li><p>Don&#8217;t use <code>@</code> pointers if you need to send data between multiple threads. Use <code>~</code> instead.</p></li>
</ol>


<p>Finally, I should note again that, if these rules are too restrictive for you (for example, if you need multiple pointers but can&#8217;t tolerate garbage collection pauses), there are more advanced solutions: borrowing, safe smart pointers, and unsafe code. But this simple system works well for many programs and forms the foundation of Rust&#8217;s approach to memory management.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[The New Borrow Check in a Nutshell]]></title>
    <link href="http://pcwalton.github.com/blog/2013/01/21/the-new-borrow-check-in-a-nutshell/"/>
    <updated>2013-01-21T17:56:00-08:00</updated>
    <id>http://pcwalton.github.com/blog/2013/01/21/the-new-borrow-check-in-a-nutshell</id>
    <content type="html"><![CDATA[<p>If you&#8217;ve used Rust for any period of time, you&#8217;ve probably been bitten by the mysterious <em>borrow check</em>—the compiler pass responsible for preventing <a href="http://stackoverflow.com/questions/6438086/iterator-invalidation-rules">iterator invalidation</a>, as well as a few other dangling pointer scenarios. The current iteration of the borrow check enforces a fairly complex set of rules. Because the rules were hard to understand and ruled out too many valid programs, we were never really satisfied with the analysis; without a simple set of rules to follow, programmers will get frustrated and give up. To remedy this, Niko has proposed a <a href="http://smallcultfollowing.com/babysteps/blog/2012/11/18/imagine-never-hearing-the-phrase-aliasable/">revamp</a> of the borrow checker known as &#8220;Imagine Never Hearing the Phrase &#8216;Aliasable, Mutable&#8217; Again&#8221;. This has mostly been implemented in <a href="https://github.com/mozilla/rust/pull/4454">a pull request</a> now, so I&#8217;d like to take the opportunity to explain the new rules. I&#8217;m particularly excited about this change because now the entire set of borrow check rules are simple enough to boil down to one principle.</p>

<p>Here&#8217;s the rule that the new borrow check is in charge of enforcing: <em>Whenever you take a pointer to an object, you may not modify that object as long as that pointer exists, except through that pointer.</em></p>

<p>(Strictly speaking, this is not all the new borrow check enforces, but the other errors the pass can produce are generally straightforward and simple dangling pointer errors. Also, I&#8217;m omitting the rules related to <code>&amp;const</code>, as this rarely-used type of pointer is likely to be removed.)</p>

<p>For unique pointers (<code>~</code>) and borrowed pointers (<code>&amp;</code>), this rule is enforced at compile time, without any runtime overhead. Here&#8217;s an example:</p>

<pre><code>let mut the_magic_word = Some(~"zap");
match the_magic_word {
    None =&gt; {}
    Some(ref word) {
        the_magic_word = None; // ERROR
        io::println(*word);
    }
}
</code></pre>

<p>Here, the line marked <code>ERROR</code> produces the error &#8220;assigning to mutable local variable prohibited due to outstanding loan&#8221;. This happens because we violated the rule above—the line <code>the_magic_word = None</code> mutates the value <code>the_magic_word</code> while there exists a pointer to it (<code>word</code>).</p>

<p>Another example:</p>

<pre><code>struct Foo {
    array: ~[int]
}

impl Foo {
    fn bar(&amp;mut self) {
        for self.array.each |i| {
            self.array = ~[];  // ERROR
            io::println(i.to_str());
        }
    }
}
</code></pre>

<p>Again, the error is &#8220;assigning to mutable field prohibited due to outstanding loan&#8221;. As before, it&#8217;s traceable to a violation of the mutation rule: the line <code>self.array = ~[]</code> mutates the <code>self.array</code> field while a pointer (<code>i</code>) into it exists.</p>

<p>This example is interesting for a couple of reasons. First of all, it illustrates the way the Rust compiler can catch iterator invalidation issues without runtime overhead in many cases: here the compiler is able to detect that the <code>i</code> iterator, which has type <code>&amp;int</code>, was invalidated, and rejects the program instead of permitting undefined behavior at runtime. Second, this example illustrates something not possible under the current borrow check regime that the new borrow check allows: namely, taking an immutable pointer to a field accessible through a <code>&amp;mut</code> pointer. (An immutable pointer is needed to call the <code>each</code> method to prevent iterator invalidation.) More than any other, this restriction probably led to the greatest number of borrow check errors in practice, since it prevented iterating over any collections reachable from <code>&amp;mut</code> pointers.</p>

<p>Now all of this works fine for <code>&amp;</code> and <code>~</code> pointers, but what about managed boxes (<code>@</code>)? It turns out that immutable <code>@</code> boxes are easy to deal with; since they can&#8217;t be mutated at all, the borrow checker doesn&#8217;t have to do anything to enforce the no-mutation rule. However, for <code>@mut</code> boxes, the situation is more complicated. For <code>@mut</code> boxes, the new borrow checker inserts <em>runtime</em> checks to enforce the pointer rules. Attempting to mutate an <code>@mut</code> box while a pointer to its contents exists results in task failure at runtime, unless the mutation is done through that pointer.</p>

<p>Interestingly, this is similar to the way various debug or safe STL implementations (for example, Microsoft&#8217;s) guard against iterator invalidation. The differences are: (1) in Rust, the checks are automatically inserted by the compiler instead of built into each collection by hand; and (2) the checks are only needed for garbage collected data, as the compiler can perform the checks at compile time for other types of data.</p>

<p>There is one gotcha here, however. As implemented, if any pointer exists to <em>any</em> part of an <code>@mut</code> box, then the <em>entire</em> box cannot be mutated while that pointer exists. This means that this example will fail:</p>

<pre><code>struct Dungeon {
    monsters: ~[Monster],
    total_gold: int
}

impl Dungeon {
    fn count_gold(@mut self) { // note `@mut self`, not `&amp;mut self`
        self.total_gold = 0;
        for self.monsters.each |monster| { // pointer created here
            self.total_gold += monster.gold;
        }
    }
}
</code></pre>

<p>Note that the iterator variable <code>monster</code> has type <code>&amp;Monster</code>. This is a pointer to the inside of <code>Dungeon</code>, so the assignment to <code>self.total_gold</code> violates the mutation rule. Unfortunately, the compiler does not currently catch this, so the program will fail at runtime.</p>

<p>There are a couple of workarounds. The simplest way is to change <code>@mut self</code> to <code>&amp;mut self</code>. Since there is no need to give out the <code>@mut</code> pointer for this operation, this is safe. Roughly speaking, the compile-time checks operate on a per-field basis, while the runtime checks operate on a per-box basis. So this change makes the operation succeed. Another possibility is to make <code>total_gold</code> into a local variable and assign to the field after the <code>for</code> loop.</p>

<p>Despite the fact that this error is easy to fix, I&#8217;m concerned about the fact that the compiler won&#8217;t catch this kind of thing at compile time. So I think we should introduce a set of warnings that looks for common violations of this rule. It&#8217;s impossible to make the warnings catch <em>all</em> failures—that&#8217;s the reason why the check is done at runtime in the first place. (In general, trying to make the compiler reason about <code>@</code> boxes is hard, since the compiler has no idea how many references to them exist.) But I suspect that we could make the analysis good enough to catch the majority of these errors in practice.</p>

<p>In any case, the take-away from all of this is that the borrow checker should be much easier and more transparent with this change. There&#8217;s essentially just one straightforward rule to remember.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[The Two Meanings of "impl"]]></title>
    <link href="http://pcwalton.github.com/blog/2012/12/30/the-two-meanings-of-impl/"/>
    <updated>2012-12-30T10:42:00-08:00</updated>
    <id>http://pcwalton.github.com/blog/2012/12/30/the-two-meanings-of-impl</id>
    <content type="html"><![CDATA[<p><code>impl</code> declarations in Rust have two forms. The subtle distinction between the two can be confusing at first, so I&#8217;ll briefly explain the difference here.</p>

<p>The first form of <code>impl</code> is a <em>type implementation</em>. (Earlier I was calling this an &#8220;anonymous trait&#8221;, but I think that this terminology is probably more confusing than it&#8217;s worth.) This form allows you to define <em>new</em> functions associated with a type. For example:</p>

<pre><code>struct Dog {
    name: ~str
}

impl Dog {
    static fn new(name: ~str) -&gt; Dog {
        return Dog { name: name };
    }

    fn speak(&amp;self) {
        io::println("woof");
    }
}
</code></pre>

<p>This example defines new functions <code>new</code> and <code>speak</code> under the <code>Dog</code> namespace. Here&#8217;s an example of their use:</p>

<pre><code>let dog = Dog::new("Snoopy");
Dog::speak(&amp;dog); // note: doesn't work today, see note below
</code></pre>

<p>(The explicit call of the form <code>Dog::speak(&amp;dog)</code> doesn&#8217;t work today, but I wrote it out to emphasize the fact that <code>speak</code> lives in the <code>Dog</code> namespace. It&#8217;s likely to work in the future, though. Today, you need to write <code>dog.speak()</code>.)</p>

<p>The second form of <code>impl</code>, on the other hand, is a <em>trait implementation</em>. It&#8217;s distinguished from the first form by the presence of a <code>:</code> followed by the name of a trait. This form allows you to provide an implementation for one or more <em>existing</em> functions belonging to a trait. It doesn&#8217;t define any new functions. For instance, suppose I defined this trait:</p>

<pre><code>trait Animal {
    static fn species(&amp;self) -&gt; ~str;
}
</code></pre>

<p>Then I can supply an implementation of <code>species()</code> for my <code>Dog</code> structure like this:</p>

<pre><code>impl Dog : Animal {
    static fn species(&amp;self) -&gt; ~str {
        return ~"Canis lupus familiaris";
    }
}
</code></pre>

<p>The key point to notice here is that this form doesn&#8217;t define any new names. This code won&#8217;t compile:</p>

<pre><code>let dog = Dog::new("Fido");
io::println(Dog::species(&amp;dog)); // unresolved name: `species`
</code></pre>

<p>But this code will:</p>

<pre><code>let dog = Dog::new("Spot");
io::println(Animal::species(&amp;dog));
</code></pre>

<p>The reason is that a trait implementation only provides the implementation of one or more <em>existing</em> functions rather than defining new functions. The function <code>species</code> is part of the <code>Animal</code> trait; it&#8217;s not part of <code>Dog</code>.</p>

<p>(You might reasonably ask: Why not duplicate the name <code>species</code> into <code>Dog</code>, for convenience? The reason is because of name collisions: it should be possible to implement <code>Animal</code> and later implement another trait with a different function called <code>species</code> without breaking existing code.)</p>

<p>So the upshot of this is that there are two forms of implementations in Rust: the type implementation, which defines new functions, and the trait implementation, which attaches functionality to existing functions. Both use the <code>impl</code> keyword, but they&#8217;re different forms with different meanings.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[A Tour of Vector Representations]]></title>
    <link href="http://pcwalton.github.com/blog/2012/12/28/a-tour-of-vectors/"/>
    <updated>2012-12-28T18:43:00-08:00</updated>
    <id>http://pcwalton.github.com/blog/2012/12/28/a-tour-of-vectors</id>
    <content type="html"><![CDATA[<p>One aspect of Rust that&#8217;s often confusing to newcomers is its treatment of strings and vectors (also known as arrays or lists). As a result of its focus on systems programming, Rust has a somewhat lower-level concept of a vector than most other languages do. As part of an overall goal to make Rust easy to understand, I thought I&#8217;d write up a quick tour of the way other languages&#8217; vectors work from the perspective of the machine in order to make it easier to map these concepts into Rust.</p>

<p>There are three common models that I&#8217;ve observed in use—for lack of better terminology, I&#8217;ll call them the Java model, the Python model, and the C++ STL model. (For brevity, I&#8217;ve omitted fixed-size, stack-allocated arrays, since these are very limited.) Most languages build upon one of these three. In a subsequent blog post, I&#8217;ll explain how Rust&#8217;s system differs from these and how the programmer can build the equivalents of each of these models in Rust.</p>

<p>We&#8217;ll start with the Java model. Java&#8217;s basic array type has a fixed size when created and cannot be changed afterward. Arrays in Java are always allocated on the Java heap. For example, consider the following line of code:</p>

<pre><code>int[] a = { 1, 2, 3, 4, 5 };
</code></pre>

<p>After this code is executed, the memory of the running program looks like this:</p>

<p><img src="http://i.imgur.com/zeztF.png" alt="image" /></p>

<p>The cell highlighted in red is the value of type <code>int[]</code>. It&#8217;s a <em>reference type</em>, which means that it represents a <em>reference</em> to the data rather than the data itself. This is important when assigning one array value to another. For instance, we execute this code:</p>

<pre><code>int[] b = a;
</code></pre>

<p>And now the memory looks like this:</p>

<p><img src="http://i.imgur.com/0t6xB.png" alt="image" /></p>

<p>Both values are pointing at the same underlying storage. We call this <em>aliasing</em> the array buffer. In Java, any number of values can point to same the underlying array storage. Because of this, the language has no idea how many pointers point to the storage at compile time; therefore, to determine when to clean up the storage, Java uses garbage collection. Periodically, the entire heap is scanned to determine whether any references to the array storage remain, and if there are none, the buffer is freed.</p>

<p>Now this model is simple and fast, but, since the arrays have a fixed size, the programmer can&#8217;t add new elements to them once they&#8217;re created. This is a very common thing to want, so Java provides another type, <code>java.util.ArrayList</code>, for this. As it turns out, the model used by Java&#8217;s <code>ArrayList</code> is essentially the same model that Python uses for all of its lists.</p>

<p>Let&#8217;s look at this model more closely. Consider this statement in Python:</p>

<pre><code>a = [ 1, 2, 3, 4, 5 ]
</code></pre>

<p>Once this is executed, the memory looks like this:</p>

<p><img src="http://i.imgur.com/xXjOU.png" alt="image" /></p>

<p>As in Java, the cell highlighted in red (<code>a</code>) is the value that actually has the Python type <code>list</code>. We can see this if we assign <code>a</code> to <code>b</code>:</p>

<pre><code>b = a
</code></pre>

<p><img src="http://i.imgur.com/jLJRj.png" alt="image" /></p>

<p>Obviously, the disadvantage of this model is that it requires two allocations instead of one. The advantage of this model is that new elements can be added to the end of the vector, and all outstanding references to the vector will see the new elements. Suppose that the vector had capacity 5 when initially created, so that no room exists to add new elements onto the end of the existing storage. Then when we execute the following line:</p>

<pre><code>b.append(6)
</code></pre>

<p>The memory looks like this:</p>

<p><img src="http://i.imgur.com/U0g4w.png" alt="image" /></p>

<p>Here, Python has created a new and larger allocation for the storage, copied the existing elements over, and freed the old allocation (indicated in gray). Because <code>a</code> and <code>b</code> both point to the <code>PyListObject</code> allocation, which has <em>not</em> changed, they both see the new elements:</p>

<pre><code>&gt;&gt;&gt; a
[1, 2, 3, 4, 5, 6]
&gt;&gt;&gt; b
[1, 2, 3, 4, 5, 6]
</code></pre>

<p>In summary, Python&#8217;s model sacrifices some efficiency at runtime because it requires both garbage collection and two allocations, but it gains flexibility by permitting both aliasing and append operations.</p>

<p>Turning our attention to the C++ STL, we find that it has a different model from both Python and Java: it sacrifices aliasing but retains the ability for vectors to grow. For instance, after this C++ STL code executes:</p>

<pre><code>std::vector a;
a.push_back(1);
a.push_back(2);
a.push_back(3);
a.push_back(4);
a.push_back(5);
</code></pre>

<p>The memory looks like this:</p>

<p><img src="http://i.imgur.com/dEQG3.png" alt="image" /></p>

<p>As before, the red box indicates the value of type <code>std::vector</code>. It is stored directly on the stack. It is still fundamentally a reference type, just as vectors in Python and Java are; note that the underlying storage does not have the type <code>std::vector&lt;int&gt;</code> but instead has the type <code>int[]</code> (a plain old C array).</p>

<p>Like Python vectors, STL vectors can grow. After executing this line:</p>

<pre><code>a.push_back(6);
</code></pre>

<p>The STL does this (assuming that there isn&#8217;t enough space to grow the vector in-place):</p>

<p><img src="http://i.imgur.com/QYFGV.png" alt="image" /></p>

<p>Just as the Python list did, the STL vector allocated new storage, copied the elements over, and deleted the old storage.</p>

<p>Unlike Java arrays, however, STL vectors do not support aliasing the contents of the vector (at least, not without some unsafe code). Instead, assignment of a value of type <code>std::vector</code> copies the contents of the vector. Consider this line:</p>

<pre><code>std::vector b = a;
</code></pre>

<p>This code results in the following memory layout:</p>

<p><img src="http://i.imgur.com/KIPIa.png" alt="image" /></p>

<p>The entire contents of the vector were copied into a new allocation. This is, as you might expect, a quite expensive operation, and represents the downside of the C++ STL approach. However, the STL approach comes with significant upsides as well: no garbage collection (via tracing GC or reference counting) is required, there is one less allocation to manage, and the vectors are allowed to grow just as Python lists are.</p>

<p>This covers the three main vector representations in use by most languages. They&#8217;re fairly standard and representative; if I didn&#8217;t mention a language here, it&#8217;s likely that its implementation uses one of these three techniques. It&#8217;s important to note that none of these are right or wrong per se—they all have advantages and disadvantages. In a future post, I&#8217;ll explain the way Rust&#8217;s vector model allows the programmer to choose the model appropriate for the task at hand.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Typestate Is Dead, Long Live Typestate!]]></title>
    <link href="http://pcwalton.github.com/blog/2012/12/26/typestate-is-dead/"/>
    <updated>2012-12-26T19:54:00-08:00</updated>
    <id>http://pcwalton.github.com/blog/2012/12/26/typestate-is-dead</id>
    <content type="html"><![CDATA[<p>One well-known fact about Rust is that the typestate system, which was one of the most unique aspects of the language early on, was dropped in Rust 0.4. The reason was that &#8220;in practice, it found little use&#8221; (courtesy of Wikipedia), which is fairly accurate. However, what&#8217;s less well known is that, in the meantime, Rust gained the building blocks necessary for typestate via its uniqueness typing system. With the right patterns, most of the safety guarantees that typestate enabled can be achieved, although it&#8217;s not as easy to use.</p>

<p>Let&#8217;s start with the simple example of a file that can be open or closed. We want to ensure at compile time that no methods that require the file to be open (for example, reading) can be called on the file while it is closed. With typestate, we would define the functions as follows:</p>

<pre><code>use core::libc;

struct File {
    descriptor: int
}

pred is_open(file: File) -&gt; bool {
    return file.descriptor &gt;= 0;
}

fn open(path: &amp;str) -&gt; File : is_open {
    let file = File { descriptor: libc::open(path) };
    check is_open(file);
    return file;
}

fn close(file: &amp;mut File) {
    libc::close(file.descriptor);
    file.descriptor = -1;
}

fn read(file: &amp;File : is_open, buf: &amp;mut [u8], len: uint) {
    libc::read(file.descriptor, ...)
}
</code></pre>

<p>And this is how this module might be used:</p>

<pre><code>fn main() {
    let file: File : is_open = open("hello.txt");
    read(&amp;file, ...);
    close(file);

    read(&amp;file, ...);    // error: expected File : is_open but found File
    check is_open(file); // will fail at runtime
}
</code></pre>

<p>The constructs here that differ from Rust of today are:</p>

<ul>
<li><p><em>Constraints</em> are special type kinds that can be attached to types with the <code>:</code> syntax; e.g. <code>File : is_open</code>.</p></li>
<li><p>The <code>pred</code> keyword declares a <em>predicate</em> function, which defines both a function and a constraint.</p></li>
<li><p>All values have unconstrained types when initially constructed. To add a constraint to a type, we use the <code>check</code> keyword. The <code>check</code> expression evaluates a predicate and fails at runtime if the predicate returns <code>false</code>; otherwise, it adds the appropriate constraint to the type of the predicate&#8217;s argument.</p></li>
</ul>


<p>Now let&#8217;s look at how we could achieve this in current Rust. We use the <em>branding pattern</em>:</p>

<pre><code>struct File&lt;State&gt; {
    priv descriptor: int,
}

// Make the type noncopyable.
impl&lt;T&gt; File&lt;T&gt; : Drop {
    fn finalize(&amp;self) {}
}

struct Open(@Open);
struct Closed(@Closed);

fn check_open&lt;T&gt;(file: File&lt;T&gt;) -&gt; File&lt;Open&gt; {
    assert file.descriptor &gt;= 0;
    let new_file: File&lt;Open&gt; = File {
        descriptor: file.descriptor
    };
    return new_file;
}

fn open(path: &amp;str) -&gt; File&lt;Open&gt; {
    let file: File&lt;Closed&gt; = File { descriptor: libc::open(path) };
    let file: File&lt;Open&gt; = check_open(file);
    return file;
}

fn close&lt;T&gt;(file: File&lt;T&gt;) -&gt; File&lt;Closed&gt; {
    let new_file: File&lt;Closed&gt; = File {
        descriptor: -1
    };
    libc::close(file.descriptor);
    return new_file;
}

fn read(file: &amp;File&lt;Open&gt;, buf: &amp;mut [u8], len: uint) {
    libc::read(file.descriptor, ...)
}
</code></pre>

<p>Using this code has a different feel to it:</p>

<pre><code>fn main() {
    let file: File&lt;Open&gt; = open("hello.txt");
    read(&amp;file, ...);
    let file: File&lt;Closed&gt; = close(file);

    read(&amp;file, ...);  // error: expected File&lt;Open&gt; but found File&lt;Closed&gt;
    let file: File&lt;Open&gt; = check_open(file); // will fail at runtime
}
</code></pre>

<p>The differences between this code and the original code using typestate are:</p>

<ul>
<li><p>Rather than directly altering the constraints attached to a value&#8217;s type, the functions that change typestate take a value of one type and return a different value of a different type. For example, <code>close()</code> takes a value of <code>File&lt;T&gt;</code> for any state <code>T</code> and returns a value of type <code>File&lt;Closed&gt;</code>.</p></li>
<li><p>Instead of the built-in notion of a predicate, this code uses a <em>phantom type</em>. A phantom type is a type for which no values can be constructed—in this example, there is no way to construct a value of type <code>Open</code> or <code>Closed</code>. Instead, these types are solely used as &#8220;markers&#8221;. In the code above, a value of type <code>File&lt;Open&gt;</code> represents an open file, and a value of type <code>File&lt;Closed&gt;</code> represents a closed file. We call these <em>branded types</em>, because <code>File</code> is <em>branded</em> with the <code>Open</code> or <code>Closed</code> status. Generics (e.g. <code>File&lt;T&gt;</code>) can be used when the state of a file is irrelevant; e.g. if a function can operate on both closed or open files.</p></li>
<li><p><code>File</code> instances are made noncopyable. This is important to prevent code like this from compiling:</p>

<pre><code>let file: File&lt;Open&gt; = open("hello.txt");
let _: File&lt;Closed&gt; = close(file); // ignore the return value
read(&amp;file, ...);  // error: use of moved value `file`
</code></pre></li>
</ul>


<p>The important idea is that to get a closed file, you must first surrender your open file. The uniqueness system in Rust allows the compiler to ensure this: when you change typestates, you must move your original value away, and the compiler will ensure that you can&#8217;t access it again.</p>

<ul>
<li><p>The file descriptor field is made private to the containing module. This is important to disallow other modules from forging open or closed <code>File</code> instances. Otherwise, other code could simply convert an open file to a closed file the same way <code>check_open</code> does:</p>

<pre><code>let open_file: File&lt;Open&gt; = open("hello.txt");
let closed_file: File&lt;Closed&gt; = close(open_file);
let fake_open_file: File&lt;Open&gt; = File { descriptor: closed_file };
// ^^^ error: use of private field 'descriptor'
read(&amp;fake_open_file, ...);
</code></pre></li>
</ul>


<p>Since the <code>File</code> structure contains a private field, no code other than the containing module can create one. In this way, we ensure that nobody can forge instances of <code>File</code> and violate our invariants.</p>

<p>Now, it&#8217;s obvious that this isn&#8217;t perfect in terms of usability. For one, it&#8217;s a design pattern, and design patterns are the sincerest form of request for syntax. I&#8217;m not particularly concerned about this aspect, however, because syntactic sugar is readily achievable with macros.</p>

<p>The issue that I&#8217;m concerned with is deeper. One nice thing about typestate as previously implemented is that you don&#8217;t have to surrender your value; you can effectively &#8220;mutate&#8221; its type &#8220;in-place&#8221;. This saves you from writing temporary variables all over the place and also saves some (cheap) copies at runtime. For example, you can write:</p>

<pre><code>let file = open("hello.txt");
read(&amp;file, ...);
close(file);
</code></pre>

<p>Instead of:</p>

<pre><code>let file = open("hello.txt");
read(&amp;file, ...);
let file = close(file);
</code></pre>

<p>In Rust, however, this causes complications, which we never fully resolved. (In fact, this is part of what led to typestate&#8217;s removal.) Suppose that <code>close</code> mutated the type of its argument to change it from <code>&amp;File&lt;Open&gt;</code> to <code>&amp;File&lt;Closed&gt;</code>. Then consider the following code:</p>

<pre><code>trait Foo {
    fn speak(&amp;self);
}

impl File&lt;Open&gt; : Foo {
    fn speak(&amp;self) {
        io::println("woof");
    }
}

trait Bar {
    fn speak(&amp;self, x: int);
}

impl File&lt;Closed&gt; : Bar {
    fn speak(&amp;self) {
        io::println("meow");
    }
}

let file = open("hello.txt");
for 5.times {
    file.speak();
    close(&amp;file);
}
</code></pre>

<p>How do we compile this code? The first time around the <code>for 5.times { ... }</code> loop, <code>file.speak()</code> should resolve to <code>Foo::speak</code>; the second time around, <code>file.speak()</code> should resolve to <code>Bar::speak</code>. Needless to say, this makes compiling extremely difficult: we would have to consider the lexical scope of every single method invocation and compile it for <em>each</em> possible predicate!</p>

<p>Because of these and other complications, mutating the type doesn&#8217;t seem possible in the general case. We would certainly need to introduce some set of restrictions—perhaps we would need to formalize the notion of a &#8220;constraint&#8221; in the type system (probably by introducing a new type kind) and then introduce some restrictions on implementation declarations to prevent instances from depending on constraints. Whatever system we come up would be pretty complex and would require a fair bit of thought to get right.</p>

<p>So I&#8217;d like to try to play with the current setup and see how far we get with it. In future versions of the language (post-1.0), it might be worthwhile to try to allow some sort of in-place &#8220;mutation&#8221; of types, similar to languages with true typestate. Overall, though, the combination of uniqueness and branding places today&#8217;s Rust in an interesting position, supporting much of the power that came with typestate in a simple system.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Unique Pointers Aren't Just About Memory Management]]></title>
    <link href="http://pcwalton.github.com/blog/2012/10/03/unique-pointers-arent-just-about-memory-management/"/>
    <updated>2012-10-03T11:32:00-07:00</updated>
    <id>http://pcwalton.github.com/blog/2012/10/03/unique-pointers-arent-just-about-memory-management</id>
    <content type="html"><![CDATA[<p>One of the most unusual features of Rust, especially when compared to languages that aren&#8217;t C++, is the three types of pointers: <em>borrowed</em> pointers (<code>&amp;T</code>), <em>unique</em> pointers (<code>~T</code>), and <em>managed</em> pointers (<code>@T</code>). Most people quite rightly ask &#8220;why three pointers? Isn&#8217;t one enough?&#8221; The usual answer is that unique pointers help with manual memory management:</p>

<ul>
<li><p>Managed pointers (<code>@T</code>) allow convenient garbage collection.</p></li>
<li><p>Unique pointers (<code>~T</code>) work like <code>malloc</code> and <code>free</code> from C to allow programmers who don&#8217;t want the overhead and complexity of GC to avoid it.</p></li>
<li><p>Borrowed pointers (<code>&amp;T</code>) allow functions to work equally well with both unique and managed pointers.</p></li>
</ul>


<p>This is all true, but there&#8217;s another, equally important reason that&#8217;s often overlooked: unique pointers allow for efficient, safe concurrency.</p>

<p>To see why, let&#8217;s consider the possible ways that an actor- or CSP-based system could enforce safe message passing. By <em>safe</em> message passing I mean that actors can&#8217;t create data races by simultaneously accessing shared mutable data. In short, we want to enforce that this adage is followed (courtesy of Rob Pike)&#8211;&#8220;do not communicate by sharing memory; share memory by communicating.&#8221;</p>

<p>There are three simple ways to do this:</p>

<ol>
<li><p>Copy all messages sent from actor to actor. Changes that one actor makes to the contents of any message do not affect the other actors&#8217; copies of the message.</p></li>
<li><p>Require that all messages sent from actor to actor be immutable. No actor may make changes to any message after it&#8217;s created.</p></li>
<li><p>Make messages inaccessible to the sender once sent&#8211;senders &#8220;give away&#8221; their messages. Only one actor may mutate a message at any given time.</p></li>
</ol>


<p>Each of these patterns has advantages and disadvantages:</p>

<ol>
<li><p>Copying all messages has the advantage that it&#8217;s simple to reason about, and the programmer doesn&#8217;t have to worry about mutability restrictions. The disadvantage is that it comes with a significant performance cost, both in terms of allocation overhead and the copying itself.</p></li>
<li><p>Requiring that messages be immutable has the advantage that many messages can be efficiently sent, but it still can lead to copying in many cases. Consider, for example, an application that spawns off a task to decode a large JPEG image. To be efficient, the image decoding algorithm generally wants to decode into a mutable buffer. But the decoded image data must be immutable to be sent, which necessitates a potentially-expensive copy of the pixel data out of the work buffer to an immutable location.</p></li>
<li><p>Making messages inaccessible to the sender has the advantage that it&#8217;s simple and fast, but it has the disadvantage that it could lead to copying if both the sender and receiver need to access the memory after the send operation.</p></li>
</ol>


<p>Because one pattern rarely fits every use case, most actor-based languages, including Rust, have varying levels of support for all three of these patterns (and for more complex patterns that don&#8217;t appear in the above list, such as <a href="http://en.wikipedia.org/wiki/Software_transactional_memory">software transactional memory</a>). However, each language tends to favor one of the three patterns &#8220;by default&#8221;. For example, Erlang leans toward option #1 (copying all messages), Clojure leans toward option #2 (immutable sharing), while Rust leans toward option #3 (giving messages away). The important thing to note here is that all of the patterns have advantages and disadvantages, and so different scenarios will call for one or the other. Consider the image decoding example from before; pattern #3 is by far the most efficient way to handle this, as the buffer needs to be mutable while the image decoder works on it, but the decoder has no need for the image after decoding is done.</p>

<p>Now the simplest way to support pattern #3 <em>safely</em>&#8211;in other words, to enforce <em>at compile time</em> that only one actor can hold onto a message at any given time&#8211;is through unique pointers. The compiler guarantees that only one reference exists to a uniquely-owned object, enforcing the property we want. Unique pointers support a <em>move</em> operation, which allows functions to &#8220;give a pointer away&#8221; to another function. So by simply requiring that the &#8220;send&#8221; method takes a unique pointer and moves its argument, we teach the compiler everything it needs to know to enforce safe concurrency.</p>

<p>In this way, unique pointers aren&#8217;t just a tool for manual memory management. They&#8217;re also a powerful tool for eliminating data races at compile time. The fact that they also allow Rust programs to avoid the garbage collector is just an added bonus.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[A Gentle Introduction to Traits in Rust]]></title>
    <link href="http://pcwalton.github.com/blog/2012/08/08/a-gentle-introduction-to-traits-in-rust/"/>
    <updated>2012-08-08T10:46:00-07:00</updated>
    <id>http://pcwalton.github.com/blog/2012/08/08/a-gentle-introduction-to-traits-in-rust</id>
    <content type="html"><![CDATA[<p>Rust traits pack a lot of flexibility into a simple system, and they&#8217;re one of my favorite features of the language. But as a result of the rapid pace of the language&#8217;s development, there&#8217;s been a fair amount of confusion as to how they work. As such, I figured I&#8217;d write up a quick tutorial explaining why and how to use them.</p>

<p>This tutorial assumes only basic knowledge of C-like languages, so I&#8217;ll try to explain everything specific to Rust that might be unclear along the way. Also note that a couple of these features are unimplemented, so if you try this today the syntax will be a little different.</p>

<h2>Simple implementations</h2>

<p>In keeping with the theme of my previous blog posts on classes, let&#8217;s start by writing a game. I&#8217;ll start by defining a struct <code>Monster</code> and a struct <code>Player</code> like this:</p>

<pre><code>struct Monster {
    name: &amp;str;      // `&amp;str` is a reference to a string
    mut health: int; // `mut` indicates that the health can be changed
}

struct Player {
    mut health: int;
}
</code></pre>

<p>Now I can create instances of each:</p>

<pre><code>fn main() {  // `fn` defines a function
    let monster = Monster {
        name: "Gelatinous Cube",
        health: 50
    };
    let player = Player {
        health: 100
    };
}
</code></pre>

<p>Without some functionality, this isn&#8217;t a particularly interesting game. So let&#8217;s add a method to <code>Monster</code>:</p>

<pre><code>impl Monster {
    fn attack(&amp;self, player: &amp;Player) {
        // fmt! is string formatting; this prints "Gelatinous Cube hits you!"
        io::println(fmt!("%s hits you!", self.name));
        player.health -= 10;
    }
}
</code></pre>

<p>And I can call it this way, inside <code>main</code>:</p>

<pre><code>monster.attack(&amp;player);
</code></pre>

<p>There are several things to note here.</p>

<ul>
<li><p>References are explicit in Rust: the <code>&amp;</code> sigil indicates that the method <code>attack</code> takes a reference to the player, not the player itself. If I didn&#8217;t write that, then the player would be copied into the method instead (and we&#8217;d get a compiler warning, because this indicates a bug).</p></li>
<li><p>I use the keyword <code>impl</code> to declare methods for a type. <code>impl</code> declarations can appear  anywhere in the module that declared the type. The <code>struct</code> and <code>impl</code> pair appears a lot in Rust code; it nicely separates out data from implementation. Objective-C and C++ programmers will find this familiar.</p></li>
<li><p>Within an implementation, functions with a <code>self</code> parameter become methods. Python programmers will find this &#8220;explicit self&#8221; familiar. Because references are explicit in Rust, you specify how <code>self</code> is supposed to be passed; in this case, by reference (<code>&amp;self</code>).</p></li>
</ul>


<h2>Generics</h2>

<p>Now that we have basic implementations covered, let&#8217;s look at something completely different: generics. (We&#8217;ll come back to implementations later on.) Like many other languages, Rust features generic functions: functions that can operate on many different types. For example, here&#8217;s a function that returns true if a vector is empty:</p>

<pre><code>// Vectors are written with square brackets around the type; e.g. a vector of
// ints is written `[int]`.
fn is_empty&lt;T&gt;(v: &amp;[T]) -&gt; bool {
    return v.len() == 0;
}
</code></pre>

<p>The generic type parameters are written inside the angle brackets (<code>&lt;</code> and <code>&gt;</code>), after the function name.</p>

<p>There&#8217;s nothing much more to say here; generics are pretty simple. In this form, however, they&#8217;re pretty limited, as we&#8217;ll see.</p>

<h2>Limitations of generics</h2>

<p>Let&#8217;s go back to our game example. Suppose I want to add functionality to save the state of the game to disk in <a href="http://en.wikipedia.org/wiki/JSON">JSON</a>. I&#8217;ll implement some methods on <code>Monster</code> and <code>Player</code> to do this:</p>

<pre><code>impl Monster {
    // `~str` means "a pointer to a string that'll be automatically freed"
    fn to_json(&amp;self) -&gt; ~str {
        return fmt!("{ name: \"%s\", health: %d }", self.name, self.health);
    }
}

impl Player {
    fn to_json(&amp;self) -&gt; ~str {
        return fmt!("{ health: %d }", self.health);
    }
}
</code></pre>

<p>Now imagine that I wanted a function to save any actor (either a monster or a player) into a file. Because monsters and players are different types, I need to use a generic function to handle both. My first attempt at the function looks like this:</p>

<pre><code>fn save&lt;T&gt;(filename: &amp;str, actor: &amp;T) {
    // Because the writer returns an error code, I use .get() to mean "require
    // that this succeeded, and abort the program if it didn't".
    let writer = io::file_writer(filename, [ io::create, io::truncate ]).get();
    writer.write(actor.to_json());
    // Because of RAII, the file will automatically be closed.
}
</code></pre>

<p>Uh-oh. This doesn&#8217;t compile. I get the following error: &#8220;attempted access of field <code>to_json</code> on type <code>&amp;T</code>, but no public field or method with that name was found&#8221;.</p>

<p>What the Rust compiler is telling me is that it doesn&#8217;t know that the type <code>T</code> in this function contains the method <code>to_json</code>. And, in fact, it might not. As written above, it&#8217;d be perfectly legal to call <code>save</code> on any type at all:</p>

<pre><code>struct Penguin {
    name: &amp;str;
}

save("penguin.txt", &amp;Penguin { name: "Fred" });
// But how do I convert penguins to JSON?
</code></pre>

<p>So I&#8217;m stuck. But Rust provides a solution: traits.</p>

<h2>Trait declaration</h2>

<p>Traits are the way to tell the Rust compiler about <em>functionality that a type must provide</em>. They&#8217;re very similar in spirit to interfaces in Java, C#, and Go, and are similar in implementation to typeclasses in Haskell. They provide the solution to the problem I&#8217;m facing: I need to tell the Rust compiler, first of all, that some types can be converted to JSON, and, additionally, for the types that can be converted to JSON, how to do it.</p>

<p>To define a trait, I simply use the <code>trait</code> keyword:</p>

<pre><code>trait ToJSON {
    fn to_json(&amp;self) -&gt; ~str;
}
</code></pre>

<p>This declares a trait named <code>ToJSON</code>, with one method that all types that implement the trait must define. That method is named <code>to_json</code>, and it takes its <code>self</code> parameter by reference.</p>

<p>Now I can define implementations of <code>ToJSON</code> for the various types I&#8217;m interested in. These implementations are exactly the same as above, except that we add <code>: ToJSON</code>.</p>

<pre><code>impl Monster : ToJSON {
    // `~str` means "a pointer to a string that'll be automatically freed"
    fn to_json(&amp;self) -&gt; ~str {
        return fmt!("{ name: \"%s\", health: %d }", self.name, self.health);
    }
}

impl Player : ToJSON {
    fn to_json(&amp;self) -&gt; ~str {
        return fmt!("{ health: %d }", self.health);
    }
}
</code></pre>

<p>That&#8217;s all there is to it. Now I can modify the <code>save</code> function so that it does what I want.</p>

<h2>Trait usage</h2>

<p>Recall that the reason why the <code>save</code> function didn&#8217;t compile is that the Rust compiler didn&#8217;t know that the <code>T</code> type contained a <code>to_json</code> method. What I need is some way to tell the compiler that this function only accepts types that contain the methods I need to call. This is accomplished through <em>trait restrictions</em>. I modify the <code>save</code> function as follows:</p>

<pre><code>fn save&lt;T:ToJSON&gt;(filename: &amp;str, actor: &amp;T) {
    let writer = io::file_writer(filename, [ io::create, io::truncate ]).get();
    writer.write(actor.to_json());
}
</code></pre>

<p>Note the addition of <code>:ToJSON</code> after the type parameter. This indicates that the function can only be called with types that implement the trait.</p>

<p>Now these calls to <code>save</code> will compile:</p>

<pre><code>save("player.txt", &amp;player);
save("monster.txt", &amp;monster);
</code></pre>

<p>But this call will not:</p>

<pre><code>save("penguin.txt", &amp;Penguin { name: "Fred" });
</code></pre>

<p>I get the error &#8220;failed to find an implementation of trait <code>ToJSON</code> for <code>Penguin</code>&#8221;, just as expected.</p>

<h2>Summing up</h2>

<p>These are the basic features of traits and comprise most of what Rust programmers will need to know. There are only a few more features beyond these, which I&#8217;ll mention briefly:</p>

<ul>
<li><p><em>Special traits</em>. Some traits are known to the compiler and represent the built-in operations. Most notably, this includes the ubiquitous <code>copy</code> trait, which invokes the copy operation that occurs when you assign with <code>let x = y</code>. You&#8217;ll see <code>T:copy</code> in many generic functions for this reason. Other special traits include <code>send</code>, which is a trait that indicates the type is sendable, and <code>add</code>, <code>sub</code>, etc, which indicate the built-in arithmetic operators. The key is that, in all cases, traits simply specify <em>what a generic type can do</em>; when you want to do something with a type parameter like <code>T</code>, you specify a trait.</p></li>
<li><p><em>Generic traits</em>. Traits can be generic, which is occasionally useful.</p></li>
<li><p><em>Default implementations</em>. It&#8217;s often helpful for traits to provide default implementations of their methods that take over when the type doesn&#8217;t provide an implementation of its own. For example, the default implementation of <code>to_json()</code> might want to use the Rust reflection API to automatically create JSON for any type, even if that type doesn&#8217;t manually implement the <code>to_json()</code> method. (Note that this feature is currently being implemented.)</p></li>
<li><p><em>Trait composition</em>. Sometimes we want one trait to include another trait. For example, the <code>Num</code> trait, which all number types in the language implement, obviously includes addition, subtraction, multiplication, etc. Trait composition allows traits to be &#8220;glued together&#8221; in this way. Note that this isn&#8217;t <em>inheritance</em>; it&#8217;s simply a convenience that allows trait methods to be combined together, like a mixin. (This is not fully implemented yet.)</p></li>
<li><p><em>First-class trait values</em>. Rarely, it&#8217;s necessary to have a trait be a first-class value, like in Java or Go, instead of attached to a generic type parameter. This doesn&#8217;t come up particularly often, but Rust does support it in the rare cases in which it&#8217;s needed. Idiomatic Rust uses generics instead of Java-like interfaces.</p></li>
</ul>


<p>That&#8217;s about all there is to traits. Traits are essentially Rust&#8217;s object system, but they&#8217;re simpler than many object systems and integrate especially well with generics.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Maximally Minimal Classes for Rust]]></title>
    <link href="http://pcwalton.github.com/blog/2012/06/03/maximally-minimal-classes-for-rust/"/>
    <updated>2012-06-03T14:35:00-07:00</updated>
    <id>http://pcwalton.github.com/blog/2012/06/03/maximally-minimal-classes-for-rust</id>
    <content type="html"><![CDATA[<p>Now that classes have been implemented as per the original proposal, the other Rusters and I have been starting to get a feel for the way they work out in practice. The results are positive, but not optimal. Although they definitely succeeded in avoiding the rigidity of traditional object-oriented languages like Java, they still have two basic problems: (1) they feel somewhat out of place with the rest of the language; and (2) they&#8217;re still too heavyweight. Nevertheless, the functionality that they enabled is important, and we shouldn&#8217;t sacrifice it.</p>

<p>Language design tends to go in cycles: we grow the language to accommodate new functionality, then shrink the language as we discover ways in which the features can be orthogonally integrated into the rest of the system. Classes seem to me to be on the upward trajectory of complexity; now it&#8217;s time to shrink them down. At the same time, we shouldn&#8217;t sacrifice the functionality that they enable.</p>

<p>In Rust, classes provide five main pieces of functionality that don&#8217;t otherwise exist: (1) nominal records; (2) constructors; (3) privacy on the field level; (4) attached methods; and (5) destructors. I&#8217;ll go over these five features in turn and discuss how each one could be simplified.</p>

<h2>Nominal records</h2>

<p>Classes in Rust are nominal records. A class in this form:</p>

<pre><code>class monster {
    let mut health: int;
    let name: str;
}
</code></pre>

<p>Is basically the moral equivalent of:</p>

<pre><code>enum monster {
    monster({
        mut health: int,
        name: str
    })
}
</code></pre>

<p>Clearly, the class form is much easier to read and much less confusing for users of the language; &#8220;enum&#8221; makes little sense as there&#8217;s nothing enumerated here. Nevertheless, there&#8217;s a bit of unnecessary noise in the form of the <code>let</code> keyword. We could simplify it to:</p>

<pre><code>class monster {
    mut health: int,
    name: str
}
</code></pre>

<p>It&#8217;s less typing, and it matches record syntax exactly.</p>

<h2>Constructors</h2>

<p>Those who have used Rust classes in their current form know that the above example class <code>monster</code> is incomplete. I still have to define a constructor for <code>monster</code>, like so:</p>

<pre><code>class monster {
    let mut health: int;
    let name: str;

    new(health: int, name: str) {
        self.health = health;
        self.name = name;
    }
}
</code></pre>

<p>This is probably the most burdensome part of classes as they currently stand&#8211;having to repeat each field name four times, and each type twice, is annoying. Many languages have solutions for this (CoffeeScript and Dart, for example), so we could consider adopting one of these languages&#8217; syntactic sugar for something like:</p>

<pre><code>class monster {
    let mut health: int;
    let name: str;

    new(self.health, self.name) {}  // sugar for the above
}
</code></pre>

<p>Unfortunately, it doesn&#8217;t stop there. Constructors have other problems. For one, there can only be one constructor per class&#8211;this is far more restrictive than Java, which permits constructor overloading. Worse, constructors can&#8217;t indicate that they failed; they can only fail the task or set some internal &#8220;this failed&#8221; flag, both of which are clearly unsatisfactory. The right way to report a recoverable error to the caller in Rust is to use the <code>result</code> type, but constructors can&#8217;t return <code>result&lt;self&gt;</code>; they can only return <code>self</code>.</p>

<p>I think the easiest way to address these problems is, following the idea that classes are just nominal records, is to abolish constructors entirely and adopt record literal syntax for initializing classes. So a class like this:</p>

<pre><code>class monster {
    mut health: int,
    name: str
}
</code></pre>

<p>Would be initialized with:</p>

<pre><code>let foe = monster {
    health: 100,
    name: "Bigfoot"
};
</code></pre>

<p>If you want to declare one or more &#8220;constructor&#8221; functions, perhaps to signal success or failure, that&#8217;s easy; they&#8217;re just functions in the same crate:</p>

<pre><code>fn monster(health: int, name: str) -&gt; result&lt;monster&gt; {
    if name == "King Kong" || name == "Godzilla" {
        ret err("Trademark violation");
    }
    ret ok(monster { health: health, name: name });
}
</code></pre>

<p>But note that you only have to write a constructor if you&#8217;re doing something special, like returning an error or initializing private fields. If your class is simple and merely holds public state, then your callers can just use the record literal syntax to create instances of the class.</p>

<h2>Privacy</h2>

<p>Classes in Rust allow private fields:</p>

<pre><code>class monster {
    let priv mut health: 100;
    let name: str;

    ...

    fn hit() {
        self.health -= 10;
    }
}
</code></pre>

<p>This is extremely useful functionality for modularity. But Rust already has a mechanism for privacy, via exports. For example, in order to write an enum whose contents are hidden from the outside world:</p>

<pre><code>enum color {
    priv red;
    priv green;
    priv blue;
}
</code></pre>

<p>(Note that the syntax here is changing; for posterity, I&#8217;m using the new syntax, but note that the code here doesn&#8217;t work at the time of this writing, as it&#8217;s not yet implemented.)</p>

<p>Only this module can construct instances of this enum, or even inspect its contents, because while the enum itself can be named, none of its variants can. So we could apply the same principle to fields of classes:</p>

<pre><code>mod A {
    mod B {
        class monster {
            priv mut health: int,
            name: str
        }

        fn hit(monster: &amp;monster) {
            monster.health -= 10;    // OK
        }
    }

    fn heal(monster: &amp;monster) {
        monster.health += 10;        // error: field "health" is private
    }
}
</code></pre>

<p>Here, a field marked with <code>priv</code> can only be named (and therefore accessed) by the enclosing module or containing modules. It works like every other instance of <code>priv</code> in the language: it restricts the use of a name to the enclosing module and its submodules.</p>

<p>It would be an error for modules that aren&#8217;t the module defining the class or an enclosing module to attempt to construct an instance of a class with a private field with the record literal syntax. This means that, if you use private fields, you need a constructor if you want your class instances to be constructible by the outside world.</p>

<h2>Methods</h2>

<p>Naturally, Rust classes support attached methods; this is much of the reason for their existence. But Rust already has a mechanism for creating methods&#8211;namely, typeclasses. We could write the above <code>monster</code> declaration this way:</p>

<pre><code>mod A {
    class monster {
        priv mut health: int,
        name: str
    }

    impl monster for &amp;monster {
        fn hit() {
            self.health -= 10;
        }
    }
}
</code></pre>

<p>The trick here is that the typeclass implementation is named <code>monster</code>, so a declaration like <code>import A::monster</code> will import both the class and the implementation. This entire scenario works because, with privacy restricted to the module, there is no need to place methods inside the class to achieve privacy.</p>

<p>Sometimes, it&#8217;s useful to have the hidden <code>self</code> parameter actually be a GC&#8217;d pointer to an instance of the class. In the original class proposal, this is accomplished with a separate type of class named <code>@class</code>. However, with this revised proposal, the <code>@class</code> functionality falls out naturally, without any extra features:</p>

<pre><code>class monster {
    priv mut health: int,
    name: str,
    friends: dvec&lt;@monster&gt;  // a dynamic vector
}

impl monster for @monster {
    fn befriend(new_friend: @monster) {
        new_friend.friends.push(self);
    }
}
</code></pre>

<p>It&#8217;d be best if we could eliminate the repetition of the <code>monster</code> name in the <code>impl</code> declaration, so I propose inferring it:</p>

<pre><code>impl for @monster {
    fn befriend(new_friend: @monster) {
        new_friend.friends.push(self);
    }
}
</code></pre>

<p>The name of the implementation would automatically be inferred to be the name of the class if, given a class C, the type is one of <code>C</code>, <code>@C</code>, <code>~C</code>, or <code>&amp;C</code>.</p>

<p>Note that, since traits can be applied to implementations, we can apply traits to classes in this way.</p>

<p>It would be ideal to eliminate the <code>impl</code> declaration entirely. However, this relies on typeclass coherence, which I&#8217;d like to keep separate to avoid coupling proposals. Nevertheless, it&#8217;s worth mentioning; so, in a forthcoming post, I&#8217;ll show how typeclass coherence can make method declaration syntax even simpler.</p>

<h2>Destructors</h2>

<p>Classes are intended to be the only mechanism for destructors in Rust. Unfortunately, there&#8217;s no obvious way to eliminate destructors from classes in a minimal way. There are a number of options:</p>

<ol>
<li><p>Keep destructors in classes, and remove resources.</p></li>
<li><p>Keep resources around, and remove destructors from classes.</p></li>
<li><p>Make the destructor interface (<code>drop</code>) into a special kind of &#8220;intrinsic interface&#8221; which enforces <em>instance coherence</em>. Then remove both resources and destructors from classes. (Recall that instance coherence means that each class can only have one implementation of an interface, which is clearly, to my mind, a necessity if destructors are to become an interface.)</p></li>
<li><p>Make <em>all</em> interfaces enforce instance coherence, make <code>drop</code> into an interface, and remove both resources and destructors from the language.</p></li>
</ol>


<p>I prefer option (4), but, as mentioned before, that&#8217;s a separate issue.</p>

<p>Finally, with nearly all of the special functionality of classes removed, it&#8217;s worth asking why records continue to exist. Indeed, I&#8217;ve been thinking for a while that structural records should be removed from the language, but the reasons for this tie into a deeper discussion on structural and nominal types and deserve their own blog post.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Coherence, modularity, and extensibility for typeclasses]]></title>
    <link href="http://pcwalton.github.com/blog/2012/05/28/coherence/"/>
    <updated>2012-05-28T22:12:00-07:00</updated>
    <id>http://pcwalton.github.com/blog/2012/05/28/coherence</id>
    <content type="html"><![CDATA[<p>I&#8217;ve been experimenting with the design of a modification to Rust typeclasses. Because it&#8217;s always best to start with code, here&#8217;s a synopsis of what I have in mind:</p>

<pre><code>mod A {
    // Declaration of an interface:
    iface to_str {
        fn to_str() -&gt; str;

        // Implementations for various types:

        impl int {
            fn to_str() -&gt; str {
                ... implementation of to_str on ints ...
            }
        }

        impl uint {
            fn to_str() -&gt; str {
                ... implementation of to_str on unsigned ints ...
            }
        }

        ... more types here ...
    }

    // Define a class and declare that it implements to_str:
    class foo : to_str {
        fn to_str() {
            ret "foo";
        }
    }
}

mod B {
    import A::to_str;    // Must import the interface first, so
                         // that the compiler can find the method
                         // "to_str".

    println(3.to_str()); // Calls the "to_str" defined above.
}

mod C {
    let x = A::foo();    // Creates a foo object named "x".

    x.to_str();          // Calls "to_str" on "x". Note that I
                         // didn't have to import the "to_str"
                         // method in this scope—since it was
                         // defined inside the declaration of the
                         // class "foo", it's obvious what the
                         // implementation is.
} 
</code></pre>

<p>Essentially, the implementations of an interface go <em>inside</em> the declaration of the interface, with one significant exception: a class is permitted to define implementations of interfaces in the body of the class. The compiler prohibits multiple implementations of the same interface on the same type using two simple rules: (1) implementations defined within an interface must be non-overlapping (i.e. there can&#8217;t be any types which match multiple implementations); and (2) a class can&#8217;t implement an interface that already defines an implementation which might itself match an instance of that class.</p>

<p>The fact that the implementations go inside the interface is a little strange—it resembles the proposed Java defender methods, although it&#8217;s used for a completely different purpose—but I believe there is an important reason for it. It means that, if a programmer wants to look up the definition of a method call, he or she can simply figure out which interface it belongs to, which must always be in scope via an <code>import</code> statement, and then look at the declaration of the interface to find the method.</p>

<p>Fundamentally, the guiding principles behind this design are that the typeclass system should be <em>coherent</em> and <em>modular</em> while supporting <em>extensibility</em>. Here are the definitions of these terms as I see them:</p>

<p><em>Coherent</em> — A typeclass system is coherent if there exists at most one implementation of an interface for every type. Typeclass systems that don&#8217;t have this property have the <em>instance coherence</em> problem (or, as we called it when we independently stumbled across it, the &#8220;hash table problem&#8221;.)</p>

<p><em>Modular</em> — A typeclass system is modular if the unit in the source code that carries the implementation for every method must be in the lexical scope of every call site that needs the implementation (or, for nominal types only, in the lexical scope of the declaration of the type). This is a little unclear, so some examples are in order. First, a simple one:</p>

<pre><code>import vec::len;
printf("The length is %u", [ 1, 2, 3 ].len());
</code></pre>

<p>In this example, we need the implementation for <code>len</code> in scope in order to make a direct call to the method <code>len</code>.</p>

<p>Now a more complex example:</p>

<pre><code>fn print_length&lt;T:len&gt;(x: T) {
    printf("The length is %u", x.len());
}

import vec::len;
print_length([ 1, 2, 3 ]);
</code></pre>

<p>Here, we need the definition of <code>len</code> in scope at the time we call <code>print_length</code>. Because <code>print_length</code> can print the length of any value that implements the <code>len</code> interface, it doesn&#8217;t intrinsically know which method to call. This information has to be provided by the caller of <code>print_length</code>. For this reason, the call to <code>print_length</code> requires the implementation <code>vec::len</code> to be in scope.</p>

<p>In typeclass systems that aren&#8217;t modular, modules that define conflicting typeclass implementations usually can&#8217;t be linked together. For instance, in such a system, if module <code>A</code> implements <code>len</code> for vectors and module <code>B</code> independently implements <code>len</code> for vectors, then modules A and B can&#8217;t be used together in the same program. Obviously, this poses a hazard for large systems composed of many independently developed submodules.</p>

<p><em>Extensibility</em> — A typeclass system facilitates extensibility if it&#8217;s possible for the programmer to introduce a new interface and provide implementations of that interface for existing types in the system. This is what makes typeclasses act like object extensions; it&#8217;s also what makes user-defined typeclasses useful on primitive types.</p>

<p>Many languages have typeclass or interface systems, but to my knowledge none of the mainstream systems support all three of these features. For example:</p>

<p><em>C++</em>—C++ concepts support extensibility, but aren&#8217;t coherent and are only somewhat modular. The C++ language permits out-of-line definition of custom operations on class and enum types. As an example, to provide an ordering on vectors of integers:</p>

<pre><code>#include &lt;vector&gt;
bool operator&lt;(std::vector&lt;int&gt; &amp;a, std::vector&lt;int&gt; &amp;b) {
    ...
}
</code></pre>

<p>In this way, C++ concepts are extensible. But there&#8217;s no check to ensure that there is only such definition in the program for each data type, so C++ concepts aren&#8217;t coherent. In this example, other namespaces can define <code>operator&lt;</code> over the same types.</p>

<p>Generally, C++ scoping rules ensure that a function can never be called outside of its lexical scope. But there is a significant exception: argument-dependent lookup. With ADL, a function can be called outside of its lexical scope if that function was defined in the same scope as the type of one of its arguments. This feature was intended for extensibility, as it allows collections like <code>std::map</code> to pick up definitions of functions like <code>operator&lt;</code> even if the functions aren&#8217;t in scope. However, it clearly comes at the cost of modularity.</p>

<p><em>Haskell</em>—Haskell typeclasses are coherent and support extensibility, but aren&#8217;t modular. Haskell programmers can define instances of typeclasses for any type in the system, but there can be only one instance of a typeclass for every type in the program. This can cause problems when two modules are linked together—if, say, module A defines <code>Show</code> of <code>int</code> and module B independently defines <code>Show</code> of <code>int</code>, modules A and B can&#8217;t be linked together.</p>

<p><em>Java</em> and <em>Go</em>—Java interfaces are modular and coherent, but aren&#8217;t extensible. In Java, an implementation of an interface can be defined only within the package that declares the type. This means, in particular, that interfaces can&#8217;t be defined on primitive types. It also means that a module can&#8217;t define an interface and then declare an implementation of the interface on existing types without modifying the existing type. Go interfaces have the same limitations (unless you define an interface over methods that are already defined on the type in question).</p>

<p><em>Scala</em>—Scala interfaces are modular but only mostly coherent; they also offer some support for extensibility. Unsurprisingly, interfaces in Scala are basically the same as interfaces in Java. The major difference is that, unlike Java, Scala provides a mechanism for extending classes with implementations of interfaces without having to modify the definition of the class—namely, implicits. This feature is extremely useful for extensibility; it also solves the problem of methods on primitive types in an elegant way. The trouble is that implicits are somewhat inconvenient to use; the programmer must define an implicit wrapper around the object, so the <code>this</code> parameter won&#8217;t refer to the object itself but rather to the wrapper. Equally importantly, implicits don&#8217;t enforce coherence—two modules can define two different implicits on the same type.</p>

<p><em>Objective-C</em>—Objective-C categories support extensibility, but aren&#8217;t modular or coherent. In Objective-C, methods can be added to existing objects by defining a new category for that object and placing the methods within that category. The problems with categories are that method calls are all late-bound (precluding static scoping), and what happens when two modules that define conflicting category implementations are linked together is <em>undefined</em>: the resulting object might provide one implementation, or it might provide the other implementation. Either way, the resulting program is unlikely to work.</p>

<p><em>Current Rust</em>—The current Rust implementation of typeclasses is modular and supports extensibility, but it doesn&#8217;t maintain coherence. Implementations are separate from interfaces in Rust (except for classes), and interfaces and implementations can both be defined over primitive types. The trouble is that there can be multiple conflicting implementations for typeclasses, which can lead to the instance coherence problem.</p>

<p>So how does this proposed design compare?</p>

<ul>
<li><p>It offers coherence because there can be only one implementation of an interface for each type. For the implementations provided within the interface itself, we can check that they&#8217;re nonoverlapping. For the implementations defined with classes, we can check to ensure that the interface implementations don&#8217;t overlap with the implementations that the interface itself defined. Either way, the checks involved are simple and ensure that we meet the criterion for coherence.</p></li>
<li><p>It offers modularity because the implementation either has to be imported as part of the interface (for implementations defined inside interfaces) or part of the nominal type (for class implementations). Consequently, it is never the case that two Rust crates cannot be linked together because of conflicting typeclass implementations.</p></li>
<li><p>It offers extensibility because, when an interface is defined, implementations can be provided for any existing types without modifying the declarations of those types.</p></li>
</ul>


<p>Finally, it supports all three of these features while maintaining a minimal feature set.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Why Lifetimes?]]></title>
    <link href="http://pcwalton.github.com/blog/2012/04/23/why-lifetimes/"/>
    <updated>2012-04-23T23:19:00-07:00</updated>
    <id>http://pcwalton.github.com/blog/2012/04/23/why-lifetimes</id>
    <content type="html"><![CDATA[<p>One of the most unique new features of Rust is its slowly-growing support for <em>regions</em>&mdash;or <em>lifetimes</em>, as some of us core developers like to call them. As lifetimes aren&#8217;t found in any mainstream languages, I thought I&#8217;d expand upon why we want them and how they can be used to improve memory management for performance (especially interactive performance) without sacrificing safety. In this first post I&#8217;ll explain why existing memory models weren&#8217;t enough and why we went searching for alternatives. Here I&#8217;m assuming basic knowledge of garbage collection, reference counting, and <code>malloc</code>/<code>free</code>, but nothing more.</p>

<p>The programming models that the current crop of mainstream programming languages expose can be divided pretty evenly into two camps: <em>explicitly-managed</em> environments and <em>garbage-collected</em> enivornments. By far, the most common programming languages built around explicitly-managed environments are C, C++, and Objective-C, and explicit memory management is so associated with these languages that it&#8217;s often just called &#8220;the C memory model&#8221;. Almost all other languages in mainstream use are garbage collected&mdash;Java, C#, JavaScript, Python, Ruby, Lisp, Perl, and tons of other languages all fall into this camp. (Note that here I&#8217;m using &#8220;garbage collection&#8221; in the general sense to mean automatic memory management; some of these languages don&#8217;t have <em>tracing</em> garbage collection and instead use reference counting.)</p>

<p>Now C and its derivatives famously offer a huge amount of control over memory usage&mdash;the built-in language features make it easy to implement stack allocation, ownership (i.e. explicit <code>new</code> and <code>delete</code>), memory pools, and reference counting (manually or with smart pointers or Objective-C&#8217;s Automatic Reference Counting). Most large C/C++/Objective-C codebases use all four strategies. Some programs (like Firefox and OS kernels) even implement their own general-purpose memory allocators. (A few use conservative garbage collectors, like the Boehm GC, but these are in the minority, so I&#8217;ll leave them aside.) This flexibility has a real benefit, especially for real-time and interactive apps (like web browsers!). Not only does explicit memory management tend to spread out the load so that pauses associated with tracing GC don&#8217;t appear, but it also provides a clear path toward improving performance whenever <code>malloc</code> and <code>free</code> do become expensive. In C++, for example, if you profile a program and see lots of expensive calls to <code>operator new</code> near the top, you can often just drop the <a href="http://www.boost.org/doc/libs/1_47_0/libs/pool/doc/index.html">Boost pool library</a>  into your code, change <code>new</code> to <code>new (pool)</code>, and call it a day.</p>

<p>Of course, all this power comes at a huge cost: namely, memory safety. Dangling pointers, wild pointers, and buffer overruns are not only annoying and costly in terms of hard-to-find bugs but also deadly from a security perspective. Heap spray attacks make any vtable dispatch on a freed object into an exploitable security vulnerability. Think about that for a second: <em>in C++, you&#8217;re always one virtual method call away from an exploitable security vulnerability</em>. You can, of course, mitigate this with sandboxing, but sandboxing has a performance and maintenance cost, and mitigating these costs isn&#8217;t easy.</p>

<p>Recognizing the huge costs associated with manual memory management, a huge amount of programming these days has shifted to languages that require garbage-collected environments. These include all of the scripting languages, as well as Java and C#. Garbage collection brings about enormous productivity savings (because the programmer doesn&#8217;t have to think as much about memory management) and also enormous security benefits. An entire class of security vulnerabilities (buffer overruns, use-after-free, stack overflow) basically cease to exist for programs running in a garbage-collected environment (to be replaced by exciting new security vulnerabilities such as SQL injection, but that&#8217;s another story).</p>

<p>The problem with garbage collection is that, now that memory management isn&#8217;t explicit (i.e. that when to recycle memory can&#8217;t be statically known by the compiler anymore), lifetimes have to be discovered at runtime&mdash;and that entails a performance cost. Tracing stop-the-world garbage collectors (and cycle collectors) have to suspend the entire program for pauses that can last hundreds of milliseconds, a fact which hurts lots of programs&mdash;for instance, mobile apps really need to be able to draw at 60 frames per second, ruling out any pause longer than 16 ms. Incremental garbage collection is better, but it&#8217;s tricky to implement and causes a loss of throughput, because the compiler has to insert extra operations on every modification of a pointer. And because everything has to essentially be done dynamically (barring simple static analyses like escape analysis), there will always be scenarios in which a fully garbage collected system loses to a manually-managed one&mdash;and both major open source web browser engines have zero tolerance for performance regressions.</p>

<p>There are many workarounds in garbage-collected languages for the lack of manual memory management. For example, <em>free lists</em> are a popular technique in languages like Java to reduce GC pause times. The idea is simple&mdash;when you have a large number of objects of the same type that you constantly allocate and deallocate, you keep a pool of old objects around and reuse them. The programmer is then responsible for manually allocating and deallocating objects from this free list. This is definitely an effective way to reduce allocations when it&#8217;s needed. But, unfortunately, there are a number of downsides to this approach.</p>

<p>First of all, garbage-collected languages usually don&#8217;t have any built-in syntax for creating objects out of a free list instead of the heap. The built-in constructor for the object can only be called on a fresh heap allocation. The usual workaround for this is to create an <code>init</code> method on the object or to create a factory object, but all of those approaches tend to look awkward syntactically. This problem itself isn&#8217;t a deal-breaker&mdash;after all, Java programmers frequently make factory classes for other reasons&mdash;but it does compound the awkwardness of the free list pattern. Of course, in and of itself, this wouldn&#8217;t be sufficient grounds to add a large amount of complexity to a garbage-collected language to support this pattern.</p>

<p>But there&#8217;s a much worse problem: <em>free lists are inherently unsafe</em>. They aren&#8217;t unsafe in the same way as C++, to be sure&mdash;in C++, there are serious security vulnerabilities to contend with&mdash;but they still allow for many of the same bugs that dangling pointers entail. To see why, notice that a free list has no idea when no more references remain to the objects that it hands out. In fact, it can&#8217;t know how many references remain to the objects allocated within it&mdash;at least, not without reference counting or tracing the object graph, which would lead back to GC and defeat the purpose of the free list! So a free list must require manual memory management. When the programmer frees an object that&#8217;s managed by a free list, it&#8217;s the programmer&#8217;s responsibility to ensure that no more references to it remain. If the programmer accidentally leaks a reference, then that object might be reused for a new instance, and a potentially hard-to-find bug will result. It&#8217;s <code>malloc</code> and <code>free</code> all over again.</p>

<p>The end result of this is that we seem to be trapped between the rock of unpredictable performance and the hard place of programmer burdens and security vulnerabilities. The current set of commonly-used languages don&#8217;t provide solutions here.</p>

<p>Fortunately, the research landscape offers some promising potential solutions, which I&#8217;ll cover next time.</p>
]]></content>
  </entry>
  
</feed>
